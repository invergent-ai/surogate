# Rust Migration Plan for Surogate

## Overview

**Hybrid approach**: All CUDA-related code stays in C++. Everything else migrates to Rust.

### What Stays in C++ (~15K lines)

**All CUDA/GPU code remains in C++ permanently**:

1. **Custom CUDA Kernels** (~10K lines)
   - `csrc/src/kernels/*.cu` - All custom kernels
   - Attention, matmul, activation, normalization
   - Quantization (FP8/FP4), fused operations
   - MoE routing and expert kernels

2. **cuBLAS/cuBLASLt** (~2K lines)
   - `csrc/src/kernels/matmul_dispatch.cu`
   - All matrix multiplication dispatch logic
   - FP8/FP4 scaling and configuration

3. **cuDNN Integration** (~1K lines)
   - Attention implementation
   - FP4 fallback paths
   - Graph API usage

4. **NCCL Communication** (~1K lines)
   - `csrc/src/training/nccl_wrapper.*`
   - Multi-GPU communicator setup
   - All-reduce, all-gather, reduce-scatter primitives

5. **CUTLASS** (~1K lines)
   - FP4 GEMM specialized kernels
   - Architecture-specific instantiations
   - Template metaprogramming

6. **CUDA Infrastructure**
   - Stream/event coordination
   - CUDA graph capture
   - Memory management helpers

### What Migrates to Rust (~79K lines)

1. **Training Loop** (~1K lines)
   - High-level epoch/step orchestration
   - Batch processing logic
   - Checkpoint coordination

2. **Weight Management** (~3K lines)
   - State machine (OnDisk → InCPU → InGPU → Gathered)
   - Prefetch coordination
   - Offloading policies (CPU/NVMe)
   - Import/export (safetensors, HuggingFace)

3. **Module System** (~31K lines)
   - Block trait definitions
   - Model composition
   - Forward/backward orchestration
   - LoRA adapter integration

4. **Configuration** (~2K lines)
   - Model config parsing (JSON → structs)
   - Runtime options
   - Recipe definitions

5. **Python Bindings** (~2K lines)
   - PyO3-based trainer class
   - Tensor marshalling (DLPack)
   - Config type exposure

6. **Optimizer** (~2K lines)
   - AdamW implementation
   - Learning rate scheduling
   - Gradient clipping

7. **Dataloader** (~1K lines)
   - Batch sampling
   - Tokenization coordination
   - Async prefetch

8. **Models** (~8K lines)
   - Model registry
   - Architecture definitions
   - Weight mapping

9. **Utilities** (~4K lines)
   - Logging/telemetry
   - CLI
   - Tensor abstractions (wrapping CUDA pointers)

10. **Testing** (~5K lines)
    - Unit tests
    - Integration tests

11. **Recipes** (~500 lines)
    - BF16, FP8, FP4 recipe implementations
    - Precision configuration

---

## FFI Architecture

```
┌─────────────────────────────────────┐
│  Python (PyO3)                      │
└─────────────────────────────────────┘
                ↓
┌─────────────────────────────────────┐
│  Rust (Orchestration)               │
│  • Training loop                    │
│  • Weight management                │
│  • Module system                    │
│  • Config/recipes                   │
└─────────────────────────────────────┘
                ↓ FFI
┌─────────────────────────────────────┐
│  C++ (GPU Execution - PERMANENT)    │
│  • CUDA kernels                     │
│  • cuBLAS/cuDNN dispatch            │
│  • NCCL communication               │
│  • CUTLASS                          │
└─────────────────────────────────────┘
```

### FFI Interface Design

**C API Layer** (new, ~500 lines):
```c
// csrc/src/binding/c_api.h

typedef struct CudaKernelHandle* CudaKernelHandle;
typedef struct CudaStreamHandle* CudaStreamHandle;

// Kernel launches
int launch_attention_forward(
    const void* q_ptr, const void* k_ptr, const void* v_ptr,
    void* output_ptr,
    int batch, int seq_len, int num_heads, int head_dim,
    CudaStreamHandle stream
);

int launch_matmul_forward(
    const void* a_ptr, const void* b_ptr, void* c_ptr,
    int m, int n, int k,
    int dtype, // 0=BF16, 1=FP8E4M3, 2=FP4
    const float* scale_a, const float* scale_b, float* scale_c,
    CudaStreamHandle stream
);

// NCCL operations
int nccl_all_reduce(
    const void* send_ptr, void* recv_ptr,
    size_t count, int dtype,
    int op, // 0=Sum, 1=Prod, 2=Max, 3=Min
    void* comm, CudaStreamHandle stream
);

// Stream management
CudaStreamHandle create_cuda_stream(void);
int synchronize_stream(CudaStreamHandle stream);
void destroy_cuda_stream(CudaStreamHandle stream);
```

**Rust FFI Bindings** (generated via bindgen):
```rust
// Generated by build.rs using bindgen
mod cuda_ffi {
    include!(concat!(env!("OUT_DIR"), "/cuda_bindings.rs"));
}

// Safe wrappers
pub struct CudaStream {
    handle: cuda_ffi::CudaStreamHandle,
}

impl CudaStream {
    pub fn new() -> Result<Self> {
        let handle = unsafe { cuda_ffi::create_cuda_stream() };
        if handle.is_null() {
            Err(Error::StreamCreationFailed)
        } else {
            Ok(Self { handle })
        }
    }

    pub async fn synchronize(&self) -> Result<()> {
        let handle = self.handle;
        tokio::task::spawn_blocking(move || {
            unsafe { cuda_ffi::synchronize_stream(handle) }
        })
        .await??;
        Ok(())
    }
}

pub fn attention_forward(
    q: &Tensor, k: &Tensor, v: &Tensor,
    output: &mut Tensor,
    stream: &CudaStream,
) -> Result<()> {
    let result = unsafe {
        cuda_ffi::launch_attention_forward(
            q.ptr(), k.ptr(), v.ptr(), output.ptr(),
            q.shape()[0] as i32, q.shape()[1] as i32,
            q.shape()[2] as i32, q.shape()[3] as i32,
            stream.handle,
        )
    };

    if result == 0 {
        Ok(())
    } else {
        Err(Error::KernelLaunchFailed(result))
    }
}
```

---

## Rust Component Design

### 1. Tensor Abstraction

```rust
pub struct Tensor {
    data: *mut c_void,
    shape: Vec<usize>,
    dtype: DType,
    device: Device,
    // Owns the memory or borrows from C++
    ownership: TensorOwnership,
}

#[derive(Debug, Clone, Copy)]
pub enum DType {
    BF16,
    FP32,
    FP8E4M3,
    FP8E5M2,
    FP4E2M1,
}

enum TensorOwnership {
    Owned(CudaAllocation),
    Borrowed,
}

impl Tensor {
    pub fn ptr(&self) -> *const c_void {
        self.data as *const c_void
    }

    pub fn ptr_mut(&mut self) -> *mut c_void {
        self.data
    }

    pub fn shape(&self) -> &[usize] {
        &self.shape
    }

    pub fn numel(&self) -> usize {
        self.shape.iter().product()
    }
}
```

### 2. Module System

```rust
pub trait Block: Send + Sync {
    type Config;
    type Weights;
    type Activations;
    type Grads;

    fn forward(
        &self,
        config: &Self::Config,
        weights: &Self::Weights,
        input: &Tensor,
        output: &mut Tensor,
        activations: &mut Self::Activations,
        stream: &CudaStream,
    ) -> Result<()>;

    fn backward(
        &self,
        config: &Self::Config,
        weights: &Self::Weights,
        activations: &Self::Activations,
        grad_output: &Tensor,
        grads: &mut Self::Grads,
        stream: &CudaStream,
    ) -> Result<()>;
}

pub struct DenseTransformerBlock {
    layer_idx: usize,
}

impl Block for DenseTransformerBlock {
    type Config = TransformerConfig;
    type Weights = DenseBlockWeights;
    type Activations = DenseBlockActivations;
    type Grads = DenseBlockGrads;

    fn forward(
        &self,
        config: &Self::Config,
        weights: &Self::Weights,
        input: &Tensor,
        output: &mut Tensor,
        activations: &mut Self::Activations,
        stream: &CudaStream,
    ) -> Result<()> {
        // Call C++ kernels via FFI
        rmsnorm_forward(&input, &weights.ln1_weight, &mut activations.ln1_output, stream)?;

        attention_forward(
            &activations.q, &activations.k, &activations.v,
            &mut activations.attn_output,
            stream,
        )?;

        matmul_forward(
            &activations.attn_output,
            &weights.attn_wo,
            &mut activations.attn_proj,
            stream,
        )?;

        // Continue forward pass...
        Ok(())
    }

    fn backward(&self, ...) -> Result<()> {
        // Call C++ kernels for backward pass
        todo!()
    }
}

pub struct ModularTransformerModel<B: Block> {
    layers: Vec<B>,
    embedding: Embedding,
    lm_head: Linear,
}

impl<B: Block> ModularTransformerModel<B> {
    pub fn forward(
        &self,
        input_ids: &Tensor,
        weights: &ModelWeights<B>,
        state: &mut RunState<B>,
        stream: &CudaStream,
    ) -> Result<Tensor> {
        // Embedding lookup
        self.embedding.forward(input_ids, &weights.embedding, &mut state.hidden, stream)?;

        // Layer-by-layer forward
        for (layer, layer_weights) in self.layers.iter().zip(&weights.layers) {
            layer.forward(
                &self.config,
                layer_weights,
                &state.hidden,
                &mut state.hidden,
                &mut state.activations[layer.layer_idx],
                stream,
            )?;
        }

        // LM head
        self.lm_head.forward(&state.hidden, &weights.lm_head, &mut state.logits, stream)?;

        Ok(state.logits.clone())
    }
}
```

### 3. Weight Management

```rust
pub enum WeightState {
    OnDisk(PathBuf),
    InCPU(CpuBuffer),
    InGPU(GpuBuffer),
    Gathered { master: GpuBuffer, work: GpuBuffer },
}

pub struct WeightManager<B: Block> {
    state: HashMap<usize, WeightState>, // layer_idx -> state
    prefetch_queue: VecDeque<usize>,
    offload_policy: OffloadPolicy,
    nccl_comm: Option<Arc<NcclComm>>,
}

impl<B: Block> WeightManager<B> {
    pub async fn prepare_layer(&mut self, layer_idx: usize, stream: &CudaStream) -> Result<LayerWeights<B>> {
        match self.state.get(&layer_idx) {
            Some(WeightState::Gathered { master, work }) => {
                // Already ready
                Ok(LayerWeights::from_buffers(master, work))
            }
            Some(WeightState::InGPU(buf)) => {
                // Need to gather (multi-GPU)
                if let Some(comm) = &self.nccl_comm {
                    let gathered = self.all_gather(buf, comm, stream).await?;
                    self.state.insert(layer_idx, WeightState::Gathered {
                        master: gathered.clone(),
                        work: gathered
                    });
                    Ok(LayerWeights::from_buffer(&gathered))
                } else {
                    Ok(LayerWeights::from_buffer(buf))
                }
            }
            Some(WeightState::InCPU(cpu_buf)) => {
                // DMA to GPU
                let gpu_buf = self.copy_to_gpu(cpu_buf, stream).await?;
                self.state.insert(layer_idx, WeightState::InGPU(gpu_buf.clone()));
                self.prepare_layer(layer_idx, stream).await
            }
            Some(WeightState::OnDisk(path)) => {
                // Load from disk
                let cpu_buf = self.load_from_disk(path).await?;
                self.state.insert(layer_idx, WeightState::InCPU(cpu_buf));
                self.prepare_layer(layer_idx, stream).await
            }
            None => Err(Error::LayerNotFound(layer_idx)),
        }
    }

    pub async fn prefetch_next_layer(&mut self, layer_idx: usize, stream: &CudaStream) -> Result<()> {
        self.prefetch_queue.push_back(layer_idx);
        tokio::spawn({
            let mut manager = self.clone();
            async move {
                manager.prepare_layer(layer_idx, stream).await
            }
        });
        Ok(())
    }
}
```

### 4. Training Loop

```rust
pub struct Trainer<B: Block> {
    model: Arc<ModularTransformerModel<B>>,
    optimizer: AdamW,
    weight_manager: WeightManager<B>,
    dataloader: DataLoader,
    config: TrainingConfig,
    stream: CudaStream,
}

impl<B: Block> Trainer<B> {
    pub async fn train_epoch(&mut self) -> Result<EpochMetrics> {
        let mut total_loss = 0.0;
        let mut num_batches = 0;

        for batch in self.dataloader.iter() {
            // Prepare batch
            let input_ids = batch.input_ids.to_device(&self.stream)?;
            let labels = batch.labels.to_device(&self.stream)?;

            // Forward pass
            let logits = self.model.forward(
                &input_ids,
                &self.weight_manager,
                &mut self.run_state,
                &self.stream,
            ).await?;

            // Loss computation (calls C++ cross-entropy kernel)
            let loss = cross_entropy_loss(&logits, &labels, &self.stream)?;

            // Backward pass
            self.model.backward(
                &loss,
                &mut self.weight_manager,
                &mut self.run_state,
                &self.stream,
            ).await?;

            // Gradient all-reduce (multi-GPU)
            if let Some(comm) = &self.nccl_comm {
                self.all_reduce_gradients(comm).await?;
            }

            // Optimizer step
            self.optimizer.step(&mut self.weight_manager)?;
            self.optimizer.zero_grad()?;

            total_loss += loss.item();
            num_batches += 1;

            // Prefetch next batch
            self.dataloader.prefetch_next().await?;
        }

        Ok(EpochMetrics {
            avg_loss: total_loss / num_batches as f32,
        })
    }
}
```

### 5. Recipe System

```rust
pub trait Recipe: Send + Sync {
    fn forward_dtype(&self) -> DType;
    fn backward_dtype(&self) -> DType;
    fn master_dtype(&self) -> DType;

    fn matmul_forward(
        &self,
        a: &Tensor,
        b: &Tensor,
        c: &mut Tensor,
        stream: &CudaStream,
    ) -> Result<()>;

    fn matmul_backward(
        &self,
        grad_output: &Tensor,
        a: &Tensor,
        b: &Tensor,
        grad_a: &mut Tensor,
        grad_b: &mut Tensor,
        stream: &CudaStream,
    ) -> Result<()>;
}

pub struct BF16Recipe;

impl Recipe for BF16Recipe {
    fn forward_dtype(&self) -> DType { DType::BF16 }
    fn backward_dtype(&self) -> DType { DType::BF16 }
    fn master_dtype(&self) -> DType { DType::FP32 }

    fn matmul_forward(&self, a: &Tensor, b: &Tensor, c: &mut Tensor, stream: &CudaStream) -> Result<()> {
        // Call C++ cuBLAS wrapper
        matmul_forward(a, b, c, DType::BF16, None, stream)
    }

    fn matmul_backward(&self, ...) -> Result<()> {
        // Call C++ cuBLAS wrapper for backward
        todo!()
    }
}

pub struct FP8HybridRecipe {
    amax_history: Vec<f32>,
    scale_inv: f32,
}

impl Recipe for FP8HybridRecipe {
    fn forward_dtype(&self) -> DType { DType::FP8E4M3 }
    fn backward_dtype(&self) -> DType { DType::FP8E5M2 }
    fn master_dtype(&self) -> DType { DType::FP32 }

    fn matmul_forward(&self, a: &Tensor, b: &Tensor, c: &mut Tensor, stream: &CudaStream) -> Result<()> {
        // Call C++ cuBLAS FP8 wrapper with scaling
        matmul_forward(
            a, b, c, DType::FP8E4M3,
            Some(&self.scale_inv),
            stream,
        )
    }
}

pub enum RecipeType {
    BF16(BF16Recipe),
    FP8Hybrid(FP8HybridRecipe),
    NVFP4(NVFP4Recipe),
}

impl RecipeType {
    pub fn from_name(name: &str) -> Result<Self> {
        match name {
            "bf16" => Ok(RecipeType::BF16(BF16Recipe)),
            "fp8-hybrid" => Ok(RecipeType::FP8Hybrid(FP8HybridRecipe::new())),
            "nvfp4" => Ok(RecipeType::NVFP4(NVFP4Recipe::new())),
            _ => Err(Error::UnknownRecipe(name.to_string())),
        }
    }
}
```

### 6. Python Bindings (PyO3)

```rust
use pyo3::prelude::*;

#[pyclass]
pub struct MultiGPUTrainer {
    inner: Arc<Mutex<Trainer<DenseTransformerBlock>>>,
    runtime: tokio::runtime::Runtime,
}

#[pymethods]
impl MultiGPUTrainer {
    #[new]
    fn new(config_path: String, num_gpus: usize) -> PyResult<Self> {
        let config = TrainingConfig::from_file(&config_path)?;
        let model = build_model(&config)?;
        let trainer = Trainer::new(model, config, num_gpus)?;

        let runtime = tokio::runtime::Builder::new_multi_thread()
            .enable_all()
            .build()?;

        Ok(Self {
            inner: Arc::new(Mutex::new(trainer)),
            runtime,
        })
    }

    fn train_step(&mut self, py: Python) -> PyResult<f32> {
        py.allow_threads(|| {
            let trainer = self.inner.lock().unwrap();
            self.runtime.block_on(async {
                trainer.train_step().await
            })
        })
    }

    fn save_checkpoint(&self, path: String) -> PyResult<()> {
        let trainer = self.inner.lock().unwrap();
        trainer.save_checkpoint(&path)?;
        Ok(())
    }
}

#[pymodule]
fn _surogate(_py: Python, m: &PyModule) -> PyResult<()> {
    m.add_class::<MultiGPUTrainer>()?;
    Ok(())
}
```

---

## Build System

### Cargo.toml

```toml
[package]
name = "surogate"
version = "0.1.0"
edition = "2021"

[lib]
name = "surogate"
crate-type = ["cdylib", "rlib"]

[dependencies]
pyo3 = { version = "0.20", features = ["extension-module", "abi3-py310"] }
tokio = { version = "1.35", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
safetensors = "0.4"
anyhow = "1.0"
thiserror = "1.0"

[build-dependencies]
bindgen = "0.69"
cc = "1.0"

[dev-dependencies]
proptest = "1.4"
```

### build.rs

```rust
use std::env;
use std::path::PathBuf;

fn main() {
    // Tell cargo to link against C++ code
    println!("cargo:rustc-link-search=native=csrc/build");
    println!("cargo:rustc-link-lib=static=surogate-cuda");

    // Link CUDA libraries
    println!("cargo:rustc-link-lib=cudart");
    println!("cargo:rustc-link-lib=cublas");
    println!("cargo:rustc-link-lib=cublasLt");
    println!("cargo:rustc-link-lib=cudnn");
    println!("cargo:rustc-link-lib=nccl");

    // Generate Rust bindings for C API
    let bindings = bindgen::Builder::default()
        .header("csrc/src/binding/c_api.h")
        .parse_callbacks(Box::new(bindgen::CargoCallbacks))
        .generate()
        .expect("Unable to generate bindings");

    let out_path = PathBuf::from(env::var("OUT_DIR").unwrap());
    bindings
        .write_to_file(out_path.join("cuda_bindings.rs"))
        .expect("Couldn't write bindings!");

    // Rerun if C API header changes
    println!("cargo:rerun-if-changed=csrc/src/binding/c_api.h");
}
```

### CMake Integration

**csrc/CMakeLists.txt** (modified):
```cmake
# Build C++ CUDA code as static library
add_library(surogate-cuda STATIC
    src/kernels/attention.cu
    src/kernels/matmul.cu
    src/kernels/activation.cu
    src/kernels/normalization.cu
    src/kernels/moe_kernels.cu
    src/kernels/quantization.cu
    src/binding/c_api.cpp  # NEW: C API for FFI
)

target_link_libraries(surogate-cuda
    cudart
    cublas
    cublasLt
    cudnn
    nccl
    CUTLASS::CUTLASS
)

# Install static library for Rust to link against
install(TARGETS surogate-cuda
    ARCHIVE DESTINATION ${CMAKE_BINARY_DIR}
)
```

---

## Migration Phases

### Phase 1: Infrastructure (1-2 months)

1. Set up Cargo workspace alongside existing CMake
2. Create C API layer (`csrc/src/binding/c_api.{h,cpp}`)
3. Configure `build.rs` with bindgen
4. Write FFI wrappers for 5-10 kernels (proof of concept)
5. Implement basic `Tensor` type in Rust
6. Test: Rust binary calls C++ kernels successfully

### Phase 2: Core Abstractions (2-3 months)

1. Port configuration parsing (`serde` + JSON)
2. Implement recipe system (traits + enums)
3. Create `CudaStream` async wrapper
4. Port model registry
5. Implement safetensors I/O
6. Test: Load model config and weights

### Phase 3: Module System (4-5 months)

1. Define `Block` trait
2. Implement `DenseTransformerBlock`
3. Port `ModularTransformerModel<B>`
4. Implement forward pass orchestration
5. Implement backward pass orchestration
6. Test: Layer-by-layer output matches C++

### Phase 4: Training Infrastructure (3-4 months)

1. Implement weight manager state machine
2. Port optimizer (AdamW)
3. Implement training loop
4. Port dataloader
5. Async prefetch coordination
6. Test: End-to-end training step

### Phase 5: Multi-GPU (2-3 months)

1. Multi-GPU weight gathering
2. Gradient all-reduce
3. ZeRO optimizer state sharding
4. Test: Multi-GPU convergence matches C++

### Phase 6: Python Bindings (2-3 months)

1. PyO3 trainer class
2. DLPack tensor marshalling
3. Python config types
4. CLI migration
5. Test: Python training script works

### Phase 7: Polish (1-2 months)

1. FFI optimization (minimize crossings)
2. Documentation (rustdoc)
3. Integration with Nsight profiling
4. Performance validation
5. Production deployment

---

## Directory Structure (Post-Migration)

```
surogate/
├── Cargo.toml                 # Rust workspace
├── csrc/
│   ├── CMakeLists.txt         # C++ build
│   ├── src/
│   │   ├── kernels/           # CUDA kernels (C++)
│   │   │   ├── attention.cu
│   │   │   ├── matmul.cu
│   │   │   ├── activation.cu
│   │   │   └── ...
│   │   ├── binding/
│   │   │   ├── c_api.h        # C API for Rust FFI
│   │   │   └── c_api.cpp
│   │   └── ...
│   └── build/
│       └── libsurogate-cuda.a # Static lib for Rust
├── src/                       # Rust code
│   ├── lib.rs
│   ├── tensor.rs
│   ├── modules/
│   │   ├── block.rs
│   │   ├── dense_block.rs
│   │   ├── moe_block.rs
│   │   └── model.rs
│   ├── training/
│   │   ├── trainer.rs
│   │   ├── optimizer.rs
│   │   └── dataloader.rs
│   ├── weights/
│   │   ├── manager.rs
│   │   ├── io.rs
│   │   └── prefetch.rs
│   ├── recipes/
│   │   ├── recipe.rs
│   │   ├── bf16.rs
│   │   ├── fp8.rs
│   │   └── fp4.rs
│   ├── config.rs
│   ├── ffi.rs                 # FFI wrappers
│   └── python.rs              # PyO3 bindings
├── build.rs                   # Build script (bindgen)
└── surogate/                  # Python package
    ├── __init__.py
    └── cli.py
```

---

## Key Technical Decisions

### 1. All GPU Code in C++

**Rationale**: No mature Rust CUDA kernel support. cuBLAS/cuDNN/NCCL dispatch logic tightly coupled to kernel launches.

**Tradeoff**: More FFI calls, but clean separation and zero risk to GPU performance.

### 2. Async Rust for Orchestration

**Rationale**: GPU operations are inherently async (stream-based). Rust async/await models this naturally.

**Pattern**:
```rust
// Launch kernel (async on GPU)
launch_kernel(&input, &output, &stream)?;
// Async wait for completion
stream.synchronize().await?;
```

### 3. Trait-Based Module System

**Rationale**: Cleaner than C++ templates for block parameterization. Better error messages.

**Example**:
```rust
// Generic over block type (monomorphized)
pub struct ModularTransformerModel<B: Block> {
    layers: Vec<B>,
}
```

### 4. Zero-Copy Tensor Abstraction

**Rationale**: Avoid data copies between Rust and C++.

**Pattern**:
```rust
pub struct Tensor {
    data: *mut c_void,  // Points to CUDA memory
    // ...
}

impl Tensor {
    pub fn ptr(&self) -> *const c_void {
        self.data
    }
}
```

Pass raw pointers across FFI; Rust manages metadata (shape, dtype).

### 5. State Machine for Weight Management

**Rationale**: Clear state transitions prevent invalid operations.

**Example**:
```rust
enum WeightState {
    OnDisk(PathBuf),
    InCPU(CpuBuffer),
    InGPU(GpuBuffer),
    Gathered { master: GpuBuffer, work: GpuBuffer },
}
```

Type system enforces: can't gather weights before loading from disk.
