---
slug: /
title: "Surogate Documentation"
---
import Figure from '@site/src/components/Figure';

# 
<div align="center" style={{backgroundColor: '#ffaf10', padding: '2rem', color: 'black'}}>
<a href="https://surogate.ai/">
<Figure width="40%"
    src="/img/logo-black.svg"
    alt="Surogate logo"
/>
</a>
<h2>LLM Pre-training, Fine-Tuning and Reinforcement Learning at practical hardware limits <br/> (C++/CUDA core, Python wrapper, BF16, FP8, NF4, NVFP4)</h2>
<br/>
<iframe src="https://ghbtns.com/github-btn.html?user=invergent-ai&repo=surogate&type=star&count=true&size=large" frameborder="0" scrolling="0" width="170" height="30" title="GitHub"></iframe>
</div>

## What is Surogate?

Surogate is an extremely fast **production-grade LLM training framework** engineered to operate at practical hardware limits, delivering nearâ€“speed-of-light throughput, low-latency execution, and predictable multi-GPU/multi-Node scaling at scale.

By combining a native **C++/CUDA execution engine**, a low-overhead [Python DSL](./about/dsl), an AOT-based [Auto Differentiantion engine](./about/automatic-differentiation.md) and a highly optimized **multi-threaded scheduler**, Surogate achieves industry-leading Speed-Of-Light (SOL) utilization on NVIDIA GPUs â€” **outperforming existing training toolkits by a wide margin**. 


## âœ¨ Highlights
Surogate is built for developers and enterprises that need fast experimentation scalability and predictable outcomes â€” whether running on-premise, in private clouds, or inside turnkey systems such as the [DenseMAX Appliance](https://www.invergent.ai/densemax-appliance).

- **ðŸ”§ Pre-training + Fine-tuning**: full fine-tuning, LoRA/QLoRA
- [**ðŸ”§ BF16, FP8 and NVFP4 Reinforcement Learning**](./guides/rl-training.md): advanced GRPO training and evaluation with custom, deterministic environments
- [**ðŸ”§ RL Environments**](./guides/rl-environments.md): predictable environments for RL training
- [**ðŸ–¥ï¸...ðŸ–¥ï¸ Native multi-GPU**](./guides/multi-gpu.md) training with the multi-threaded backend
- [**ðŸ–¥ï¸...ðŸ–¥ï¸ Native multi-Node**](./guides/multi-node.md) DDP training with Ray
- **âš¡ Native C++/CUDA engine** for nearâ€“Speed-Of-Light (SOL) throughput
- [**ðŸ”¥ Python DSL**](./about/dsl.md) with AOT auto-differentiation for adding new model architectures
- [**âš–ï¸ Smart CPU Offloading**](./guides/offloading.md) for weights, gradients, activations, quants
- **ðŸ“œ Pre-built training recipes**: 
  - [**ðŸ’Ž BF16**](./guides/precision-and-recipes.md#bf16): Baseline recipe using `bfloat16` for all GEMMs, designed for maximum numerical accuracy. No quantization is applied.
  - [**ðŸ”¥ FP8**](./guides/precision-and-recipes.md#fp8-hybrid): Native `FP8` training delivering extreme performance with `E4M3` used for activations and weights and `E5M2` for gradients. Uses per-tensor delayed scaling to provide stable training.
  - [**ðŸ”¥ NVFP4**](./guides/precision-and-recipes.md#fp4-nvfp4): Native CUTLASS `FP4 E2M1` training with two-level block scaling for extreme performance and memory efficiency on Blackwell GPUs (**SM100+**: B200, B300, RTX 50xx series). Uses stochastic rounding and random Hadamard Transforms for numerical stability. **Supports NVIDIA B200, B300, RTX 5070, 5080, 5090 !!**
- [**âš¡ BnB/FP8/NVFP4 QLoRA**](./guides/qlora.md) Support for a variety of QLoRA configurations, including online quantization (FP8, NVFP4, BnB) or loading pre-quantized weights (FP8, NVFP4)
- [**ðŸ‘Œ Optimizers**](./guides/optimizers.md): AdamW 8bit, !! NorMuon !!
- **ðŸ–¥ï¸ Runs on all NVIDIA GPUs**: sm80, sm86, sm89, sm90, sm100, sm103, sm120, sm121
- [**ðŸ§ª Mixed-precision training**](./guides/precision-and-recipes.md#mixed-precision-training): Mix different dtypes for GEMMs, model, gradients and LoRA recipes to create your own flavor.
- [**ðŸ§¬ Adaptive Training**](./about/adaptive-training.md): built-in automated training monitoring with automatic phase detection, multi-criteria early stopping (convergence, compute-efficiency, divergence, plateau), auto LR management, MoE imbalance detection, Chinchilla token budgeting and dynamic epoch adjustment
- [**ðŸŽ¨ Dedicated MoE Features**](./guides/moe.md): Expert Parallelism, Least-Loaded EP load-balancing, MoE training metrics, Imbalance detection
- **ðŸ¥ž Stacked LoRA training**: Train a LoRA adapter on top of another LoRA adapter to skip offline merging into base model.
- **ðŸ›¡ï¸ Designed for reliability**: deterministic configs, explicit recipes, and a clear C++ core
- **ðŸ§  Supported models**: Qwen2.5, Qwen3, Qwen3 MoE, Llama 3+, Nemotron Nano. Models can be added easily, please create a PR if you need a specific model.


## Quickstart

### Option A: Run using Docker (recommended)
Surogate provides 3 docker images for various CUDA versions. Currently only the `x86-64` architecture is supported.

| CUDA   | Image                                        | Recommended NVIDIA Driver | Minimum NVIDIA Driver |
| ------ | -------------------------------------------- | ------------------------- | --------------------- |
| 12.8.1 | `ghcr.io/invergent-ai/surogate:latest-cu128` | `>= 570.124.06`           | `>= 525`              |
| 12.9.1 | `ghcr.io/invergent-ai/surogate:latest-cu129` | `>= 575.57.08`            | `>= 525`              |
| 13.1   | `ghcr.io/invergent-ai/surogate:latest-cu13`  | `>= 590.48.01`            | `>= 580`              |

```bash
docker run --gpus=all -v /my/local/config.yaml:/home/surogate/config.yaml -v /my/local/output_dir:<OUTPUT_DIR_FROM_CONFIG_YAML> <IMAGE> sft config.yaml
```

### Option B: Install via script
```bash
curl -LsSf https://surogate.ai/install.sh | sh
```

Follow these guides to run your first training:

- [Installation](getting-started/installation.md)
- [Training modes: Pretraining vs Full Fine-Tuning vs LoRA](getting-started/training-modes.md)
- [Quickstart: SFT](getting-started/quickstart-sft.md)
- [Quickstart: Pre-training](getting-started/quickstart-pretraining.md)
- [Quickstart: GRPO](getting-started/quickstart-grpo.md)

## Hardware / Requirements

- NVIDIA GPU + recent driver
- CUDA **12.8, 12.9, 13**, NCCL, cuDNN
- Linux x86_64

### Supported NVIDIA GPUs:
- `SM80`: A100, A30
- `SM86`: A2, A16, A10, A40, RTX3050, RTX3060, RTX 3070, RTX 3080, RTX 3090, A2000, A3000, A4000, A5000, A6000
- `SM89`: L4, L40, L40S, RTX 4050, RTX 4060, RTX 4070, RTX 4080, RTX 4090, RTX 2000 Ada, RTX 4000 SFF Ada, RTX 4000 Ada, RTX 4500 Ada, RTX 5000 Ada, RTX 6000 Ada
- `SM90`: H100, H200, GH200
- `SM100`: B200, GB200
- `SM103`: B300, GB300
- `SM120`: RTX PRO 6000/5000/4000/2500/2000 Blackwell,  RTX 5050,  RTX 5060,  RTX 5070,  RTX 5080,  RTX 5090
- `SM121`: DGX Spark

## Learn More

- **[How Surogate Works](about/how-it-works.md)**: Deep dive into the C++/CUDA engine and multi-threaded scheduler.
- **[Examples Library](examples/index.md)**: Pre-built configurations for Qwen, Llama, and MoE models.
- **[User Guides](guides/configuration.md)**: Advanced documentation on precision, memory, scaling, and more.
- **[Technical Reference](reference/config.md)**: Comprehensive CLI and API reference.
