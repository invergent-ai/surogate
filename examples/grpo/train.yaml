# GRPO RL Training Example

# model: "Qwen3-0.6B-NVFP4"
model: "./reverse-fft-NVFP4"
output_dir: ./outputs
gpus: 1

per_device_train_batch_size: 1
sequence_len: 2048

# max_steps = orchestrator steps (must match orch.yaml max_steps)
max_steps: 20
logging_steps: 1

learning_rate: 2e-4
lr_scheduler_type: constant
warmup_steps: 0
max_grad_norm: 1.0
weight_decay: 0.01

recipe: fp8-hybrid

lora: true
lora_rank: 16
lora_alpha: 32
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# GRPO loss
loss:
  ratio_type: token
  kl_tau: 0.0
  adv_tau: 1.0
  token_mask_low: 0.125
  token_mask_high: 8.0
  geo_mask_low: 0.1
  geo_mask_high: 10.0

# QeRL noise scheduler (Adaptive Quantization Noise)
# Adds Gaussian noise to RMSNorm weights in inference model for exploration.
noise_scheduler:
  enabled: true
  sigma_start: 5e-2
  sigma_end: 5e-4
  num_stages: 10

# Checkpointing
save_steps: 100
checkpoint_dir: ./outputs/checkpoints
