model: Qwen/Qwen3-0.6B
output_dir: ./output
merge_adapter: false
apply_recommended_values: false

per_device_train_batch_size: 2
gradient_accumulation_steps: 4

sample_packing: true
sequence_len: 2048
truncation_strategy: split
use_chat_template: false
loss_scale: all

num_epochs: 10
eval_steps: 100
save_steps: 0

learning_rate: 6e-4
lr_scheduler_type: cosine
final_lr_fraction: 0.1
warmup_steps: 10
max_grad_norm: 1.0

recipe: fp8-hybrid
optimizer: adamw_8bit
gpus: 4
lora: false

dataloader_num_workers: 1
datasets:
  - path: "winglian/tiny-shakespeare"
    type: text

# report_to: aim
# aim_experiment: qwen3-0.6b-fft
# aim_repo: /home/densemax2/work/flavius/surogate/.aim
# aim_name: fft-bf16-adamw