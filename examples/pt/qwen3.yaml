model: ./Qwen3-0.6B
output_dir: ./output

per_device_train_batch_size: 2
gradient_accumulation_steps: 2

sample_packing: false
sequence_len: 2048
truncation_strategy: split
use_chat_template: false
loss_scale: all

max_steps: 500
eval_steps: 50
save_steps: 250

learning_rate: 6e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
max_grad_norm: 1.0

recipe: fp8-hybrid
optimizer: normuon
gpus: 1
lora: false

dataloader_num_workers: 2
datasets:
  - path: "Trelis/tiny-shakespeare"
    split: train
    type: text
    text_field: Text
validation_datasets:
  - path: "Trelis/tiny-shakespeare"
    split: test
    type: text
    text_field: Text
