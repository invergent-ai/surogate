model: PrimeIntellect/Qwen3-0.6B
output_dir: ./reverse-fft-2
model_type: qwen3_thinking

per_device_train_batch_size: 2
gradient_accumulation_steps: 64

sample_packing: true
sequence_len: 4096
save_steps: 0

max_steps: 100
eval_steps: 25
gpus: 1

learning_rate: 3e-5
lr_scheduler_type: constant

recipe: fp8-hybrid

lora: false
recompute: true

dataloader_num_workers: 4
datasets:
  - path: "willcb/R1-reverse-wikipedia-paragraphs-v1-1000"
    type: conversation
    messages_field: prompt
    completion_field: completion
