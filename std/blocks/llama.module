import std.primitives.matmul
import std.primitives.rmsnorm
import std.primitives.rope
import std.primitives.attention
import std.primitives.activations
block LlamaBlock(
  d_model: int,
  num_query_heads: int,
  num_kv_heads: int,
  head_size: int,
  d_ff: int,
  max_seq: int,
  eps: float = 1e-6
):
  """
  LLaMA-style dense transformer block (pre-norm, SwiGLU MLP, RoPE GQA attention).
  """
  let:
    C = d_model
    Hq = num_query_heads
    Hkv = num_kv_heads
    D = head_size
    QKV = (Hq + 2 * Hkv) * D
    AttnDim = Hq * D
    M = d_ff
    MUp = 2 * M
  params:
    ln1_weight: [C]
    ln2_weight: [C]
    qkv_weight: [QKV, C]
    out_weight: [C, AttnDim]
    rope_freqs: [max_seq, D // 2, 2, fp32] @frozen
    mlp_up_weight: [MUp, C]
    mlp_down_weight: [C, M]
  forward:
    inputs:
      x: [B, T, C]
      residual: [B, T, C]
      position_ids: [T, int32]
    outputs:
      out: [B, T, C]
      residual_out: [B, T, C]
    graph:
      (residual, x, ln1_weight) -> fused_residual_rmsnorm(eps=eps) -> (residual_mid, ln1_out, _)
      ln1_out -> view(shape=[B * T, C]) -> ln1_flat
      (ln1_flat, qkv_weight) -> matmul(transpose=NT) -> qkv_flat
      qkv_flat -> view(shape=[B, T, Hq + 2 * Hkv, D]) -> qkv_packed
      (qkv_packed, rope_freqs, position_ids) -> rope(rotary_dim=D) -> qkv_rope
      qkv_rope -> flash_attention(causal=true) -> (att, _)
      att -> view(shape=[B * T, AttnDim]) -> att_flat
      (att_flat, out_weight) -> matmul(transpose=NT) -> att_out_flat
      att_out_flat -> view(shape=[B, T, C]) -> att_out
      (residual_mid, att_out, ln2_weight) -> fused_residual_rmsnorm(eps=eps) -> (residual_out, ln2_out, _)
      ln2_out -> view(shape=[B * T, C]) -> ln2_flat
      (ln2_flat, mlp_up_weight) -> matmul(transpose=NT) -> mlp_up_flat
      mlp_up_flat -> view(shape=[B, T, MUp]) -> mlp_up
      mlp_up -> swiglu() -> mlp_act
      mlp_act -> view(shape=[B * T, M]) -> mlp_act_flat
      (mlp_act_flat, mlp_down_weight) -> matmul(transpose=NT) -> out_flat
      out_flat -> view(shape=[B, T, C]) -> out
