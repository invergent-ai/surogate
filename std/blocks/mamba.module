import std.primitives.matmul
import std.primitives.rmsnorm
import std.primitives.mamba
import std.primitives.activations
import std.primitives.bias
block MambaBlock(
  d_model: int,
  mamba_num_heads: int,
  mamba_head_dim: int,
  mamba_state: int,
  mamba_conv_kernel: int,
  mamba_groups: int = 1,
  mamba_chunk_size: int = 256,
  mamba_intermediate: int = 0,
  eps: float = 1e-6,
  use_bias: bool = false,
  use_conv_bias: bool = false
):
  """
  Mamba/SSM block with pre-norm residual stream.
  """
  let:
    C = d_model
    H = mamba_num_heads
    Hd = mamba_head_dim
    Dm = mamba_intermediate if mamba_intermediate > 0 else H * Hd
    conv_dim = Dm + 2 * mamba_groups * mamba_state
    proj_size = Dm + conv_dim + H
  params:
    ln1_weight: [C]
    in_proj_weight: [proj_size, C]
    in_proj_bias: [proj_size] if use_bias
    out_proj_weight: [C, Dm]
    out_proj_bias: [C] if use_bias
    conv1d_weight: [conv_dim, 1, mamba_conv_kernel]
    conv1d_bias: [conv_dim] if use_conv_bias
    A_log: [H, fp32]
    D: [H, fp32]
    dt_bias: [H, fp32]
    norm_weight: [Dm]
  forward:
    inputs:
      x: [B, T, C]
      residual: [B, T, C]
    outputs:
      out: [B, T, C]
      residual_out: [B, T, C]
    graph:
      (residual, x, ln1_weight) -> fused_residual_rmsnorm(eps=eps) -> (residual_out, ln1_out, _)
      ln1_out -> view(shape=[B * T, C]) -> ln1_flat
      (ln1_flat, in_proj_weight) -> matmul(transpose=NT) -> proj_flat
      if use_bias:
        proj_flat -> view(shape=[B, T, proj_size]) -> proj_tmp
        (proj_tmp, in_proj_bias) -> bias_add() -> proj
      else:
        proj_flat -> view(shape=[B, T, proj_size]) -> proj
      proj -> mamba_split_proj(d_inner=Dm, conv_dim=conv_dim, num_heads=H, head_dim=Hd) -> (gate, conv_in, delta)
      if use_conv_bias:
        conv1d_bias -> conv_bias
      else:
        zeros(shape=[conv_dim], dtype=bf16) -> conv_bias
      (conv_in, conv1d_weight, conv_bias) -> mamba_causal_conv1d(kernel_size=mamba_conv_kernel, silu=true) -> (conv_out, _)
      conv_out -> mamba_split_conv_out(d_inner=Dm, groups=mamba_groups, d_state=mamba_state) -> (u, B_ssm, C_ssm)
      A_log -> mamba_expand_A(num_heads=H, head_dim=Hd, d_state=mamba_state) -> A_exp
      D -> mamba_expand_head_param(num_heads=H, head_dim=Hd) -> D_exp
      dt_bias -> mamba_expand_head_param(num_heads=H, head_dim=Hd) -> dt_bias_exp
      zeros(shape=[B, Dm, mamba_state], dtype=bf16) -> ssm_state
      (u, delta, A_exp, B_ssm, C_ssm, D_exp, dt_bias_exp, ssm_state) -> mamba_selective_scan(d_state=mamba_state, groups=mamba_groups, chunk_size=mamba_chunk_size) -> (scan_out_bdt, _)
      scan_out_bdt -> mamba_transpose_bdt_to_btd() -> scan_out
      (gate, scan_out) -> silu_mul() -> gated
      (gated, norm_weight) -> mamba_group_rmsnorm(eps=eps, groups=mamba_groups) -> (normed, _)
      normed -> view(shape=[B * T, Dm]) -> normed_flat
      (normed_flat, out_proj_weight) -> matmul(transpose=NT) -> out_flat
      out_flat -> view(shape=[B, T, C]) -> out_tmp
      if use_bias:
        (out_tmp, out_proj_bias) -> bias_add() -> out
      else:
        out_tmp -> out
