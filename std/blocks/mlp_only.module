import std.primitives.matmul
import std.primitives.rmsnorm
import std.primitives.activations
block MLPOnlyBlock(
  d_model: int,
  d_ff: int,
  eps: float = 1e-6,
  activation: enum(SwiGLU, SiLU, ReLU2) = SwiGLU
):
  """
  MLP-only block with pre-norm residual stream.
  """
  let:
    C = d_model
    M = d_ff
    MUp = 2 * M if activation == SwiGLU else M
  params:
    ln1_weight: [C]
    mlp_up_weight: [MUp, C]
    mlp_down_weight: [C, M]
  forward:
    inputs:
      x: [B, T, C]
      residual: [B, T, C]
    outputs:
      out: [B, T, C]
      residual_out: [B, T, C]
    graph:
      (residual, x, ln1_weight) -> fused_residual_rmsnorm(eps=eps) -> (residual_out, ln1_out, _)
      ln1_out -> view(shape=[B * T, C]) -> ln1_flat
      (ln1_flat, mlp_up_weight) -> matmul(transpose=NT) -> mlp_up_flat
      mlp_up_flat -> view(shape=[B, T, MUp]) -> mlp_up
      if activation == SwiGLU:
        mlp_up -> swiglu() -> mlp_act
      else:
        if activation == SiLU:
          mlp_up -> silu() -> mlp_act
        else:
          mlp_up -> relu2() -> mlp_act
      mlp_act -> view(shape=[B * T, M]) -> mlp_act_flat
      (mlp_act_flat, mlp_down_weight) -> matmul(transpose=NT) -> out_flat
      out_flat -> view(shape=[B, T, C]) -> out
