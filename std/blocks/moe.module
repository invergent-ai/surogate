import std.primitives.matmul
import std.primitives.rmsnorm
import std.primitives.rope
import std.primitives.attention
import std.primitives.activations
import std.primitives.bias
import std.primitives.moe
block MoETransformerBlock(
  d_model: int,
  num_query_heads: int,
  num_kv_heads: int,
  head_size: int,
  d_ff: int,
  num_experts: int,
  top_k: int = 2,
  max_seq: int,
  eps: float = 1e-6,
  use_qkv_bias: bool = false,
  use_qk_norm: bool = false,
  activation: enum(SwiGLU, SiLU, ReLU2) = SwiGLU,
  use_sigmoid: bool = false,
  norm_topk_prob: bool = false,
  routed_scaling_factor: float = 1.0,
  use_router_bias: bool = false,
  expert_ff: int = 0
):
  """
  Attention + MoE block (router + experts) with pre-norm residual stream.
  """
  let:
    C = d_model
    Hq = num_query_heads
    Hkv = num_kv_heads
    D = head_size
    QKV = (Hq + 2 * Hkv) * D
    AttnDim = Hq * D
    Dexp = expert_ff if expert_ff > 0 else d_ff
    MUp = 2 * Dexp if activation == SwiGLU else Dexp
  params:
    ln1_weight: [C]
    ln2_weight: [C]
    qkv_weight: [QKV, C]
    qkv_bias: [QKV] if use_qkv_bias
    out_weight: [C, AttnDim]
    q_norm_weight: [D] if use_qk_norm
    k_norm_weight: [D] if use_qk_norm
    rope_freqs: [max_seq, D // 2, 2, fp32] @frozen
    router_weight: [num_experts, C]
    router_bias: [num_experts] if use_router_bias
    experts_gate_up: [num_experts, MUp, C]
    experts_down: [num_experts, C, Dexp]
  forward:
    inputs:
      x: [B, T, C]
      residual: [B, T, C]
      position_ids: [T, int32]
    outputs:
      out: [B, T, C]
      residual_out: [B, T, C]
    graph:
      (residual, x, ln1_weight) -> fused_residual_rmsnorm(eps=eps) -> (residual_mid, ln1_out, _)
      ln1_out -> view(shape=[B * T, C]) -> ln1_flat
      (ln1_flat, qkv_weight) -> matmul(transpose=NT) -> qkv_flat
      if use_qkv_bias:
        qkv_flat -> view(shape=[B, T, QKV]) -> qkv_tmp
        (qkv_tmp, qkv_bias) -> bias_add() -> qkv_biased
        qkv_biased -> view(shape=[B, T, Hq + 2 * Hkv, D]) -> qkv_packed
      else:
        qkv_flat -> view(shape=[B, T, Hq + 2 * Hkv, D]) -> qkv_packed
      if use_qk_norm:
        (qkv_packed, q_norm_weight, k_norm_weight, rope_freqs, position_ids) -> qkv_qk_norm_rope(eps=eps) -> (qkv_rope, _, _)
      else:
        (qkv_packed, rope_freqs, position_ids) -> rope(rotary_dim=D) -> qkv_rope
      qkv_rope -> flash_attention(causal=true) -> (att, _)
      att -> view(shape=[B * T, AttnDim]) -> att_flat
      (att_flat, out_weight) -> matmul(transpose=NT) -> att_out_flat
      att_out_flat -> view(shape=[B, T, C]) -> att_out
      (residual_mid, att_out, ln2_weight) -> fused_residual_rmsnorm(eps=eps) -> (residual_out, ln2_out, _)
      ln2_out -> view(shape=[B * T, C]) -> ln2_flat
      (ln2_flat, router_weight) -> matmul(transpose=NT) -> router_logits
      if use_router_bias:
        router_logits -> view(shape=[B, T, num_experts]) -> logits_tmp
        (logits_tmp, router_bias) -> bias_add() -> logits_biased
        logits_biased -> view(shape=[B * T, num_experts]) -> router_logits_b
      else:
        router_logits -> router_logits_b
      if use_sigmoid:
        router_logits_b -> moe_sigmoid() -> router_probs
      else:
        router_logits_b -> moe_softmax() -> router_probs
      router_probs -> moe_topk(top_k=top_k, normalize=norm_topk_prob) -> (routing_weights, expert_indices)
      if routed_scaling_factor != 1.0:
        routing_weights -> moe_scale(scale=routed_scaling_factor) -> routing_scaled
      else:
        routing_weights -> routing_scaled
      expert_indices -> moe_compute_expert_counts(num_experts=num_experts, top_k=top_k) -> expert_counts
      expert_counts -> moe_compute_expert_offsets(num_experts=num_experts) -> expert_offsets
      zeros(shape=[num_experts], dtype=int32) -> expert_positions
      (expert_indices, expert_offsets, expert_positions) -> moe_build_indices(num_experts=num_experts, top_k=top_k) -> (gather_indices, scatter_indices)
      (ln2_flat, gather_indices) -> moe_permute(top_k=top_k) -> permuted_input
      if activation == SwiGLU:
        (permuted_input, experts_gate_up, expert_offsets) -> moe_grouped_gemm_gate_up() -> gate_up
        gate_up -> swiglu() -> expert_act
      else:
        (permuted_input, experts_gate_up, expert_offsets) -> moe_grouped_gemm() -> expert_pre
        if activation == SiLU:
          expert_pre -> silu() -> expert_act
        else:
          expert_pre -> relu2() -> expert_act
      (expert_act, experts_down, expert_offsets) -> moe_grouped_gemm_down() -> expert_out
      (expert_out, routing_scaled, scatter_indices) -> moe_unpermute(top_k=top_k) -> combined
      combined -> view(shape=[B, T, C]) -> out
