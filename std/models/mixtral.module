# Mixtral Model Architecture
# Maps to: csrc/src/models/qwen3moe/ (similar MoE structure)

"""
Mixtral (Mixture-of-Experts) model architecture.
Sparse MoE variant of Mistral.
"""

import std.primitives
import std.modules.{Linear}
import std.modules.attention.gqa.{CausalSelfAttention}
import std.modules.moe.{SwitchRouter, MoEExperts}
import std.blocks.moe.{MoETransformerBlock}

# =============================================================================
# Mixtral - Mixture-of-Experts Model
# =============================================================================

model Mixtral(
  vocab_size: int = 32000,
  d_model: int = 4096,
  n_layers: int = 32,
  num_heads: int = 32,
  num_kv_heads: int = 8,
  d_ff: int = 14336,
  num_experts: int = 8,
  top_k: int = 2,
  max_seq: int = 32768,
  eps: float = 1e-5,
  rope_theta: float = 1000000.0,
  sliding_window: int? = None,
  tie_word_embeddings: bool = false,
  aux_loss_alpha: float = 0.01
):
  """
  Mixtral Mixture-of-Experts model architecture.

  Key features:
  - Sparse MoE replacing dense MLP
  - Top-K expert routing (typically top-2)
  - GQA for attention
  - Optional sliding window attention

  Configurations:
  - Mixtral-8x7B: d_model=4096, n_layers=32, num_experts=8, top_k=2

  Active parameters per token: ~12.9B (2 experts active out of 8)
  Total parameters: ~46.7B

  Maps to: models/qwen3moe/ (similar structure)
  """

  let:
    d_head = d_model // num_heads

    constraint:
      d_model % num_heads == 0, "d_model must be divisible by num_heads"
      num_heads % num_kv_heads == 0, "num_heads must be divisible by num_kv_heads"
      top_k <= num_experts, "top_k must be <= num_experts"

  params:
    embedding: [vocab_size, d_model]
    blocks: [n_layers] x MoETransformerBlock(
      d_model=d_model,
      num_heads=num_heads,
      num_kv_heads=num_kv_heads,
      d_ff=d_ff,
      num_experts=num_experts,
      top_k=top_k,
      max_seq=max_seq,
      rope_theta=rope_theta,
      eps=eps,
      use_qk_norm=false,
      activation=swiglu,
      aux_loss_alpha=aux_loss_alpha,
      use_shared_expert=false
    )
    final_norm: [d_model]
    lm_head: tied_to(embedding) if tie_word_embeddings else [vocab_size, d_model]

  forward:
    in: token_ids: [B, T, int32]
    out: (logits: [B, T, vocab_size], aux_loss: float)

    graph:
      token_ids -> embedding(embedding) -> x0
      zeros([B, T, d_model]) -> residual0

      # Forward through MoE blocks, accumulating aux loss
      (x0, residual0) -> StackedMoEBlocks(blocks, n_layers,
        attention_config={window_size: sliding_window} if sliding_window else {}
      ) -> (xN, residualN, total_aux_loss)

      (residualN, xN, final_norm) -> fused_residual_rmsnorm(eps=eps) -> (_, xF, _)
      (xF, lm_head) -> matmul(transpose=TN) -> logits

  hf_config:
    architecture: "MixtralForCausalLM"
    config_class: "MixtralConfig"
    param_mapping:
      d_model: hidden_size
      n_layers: num_hidden_layers
      num_heads: num_attention_heads
      num_kv_heads: num_key_value_heads
      d_ff: intermediate_size
      vocab_size: vocab_size
      max_seq: max_position_embeddings
      eps: rms_norm_eps
      rope_theta: rope_theta
      num_experts: num_local_experts
      top_k: num_experts_per_tok
      sliding_window: sliding_window

  hf_mapping:
    embedding: "model.embed_tokens.weight"
    final_norm: "model.norm.weight"
    lm_head: "lm_head.weight"

    # Attention weights (same as dense)
    blocks[{i}].ln1_weight: "model.layers.{i}.input_layernorm.weight"
    blocks[{i}].qkv_weight: fuse(
      "model.layers.{i}.self_attn.q_proj.weight",
      "model.layers.{i}.self_attn.k_proj.weight",
      "model.layers.{i}.self_attn.v_proj.weight",
      dim=0
    )
    blocks[{i}].out_weight: "model.layers.{i}.self_attn.o_proj.weight"
    blocks[{i}].ln2_weight: "model.layers.{i}.post_attention_layernorm.weight"

    # MoE weights
    blocks[{i}].router_weight: "model.layers.{i}.block_sparse_moe.gate.weight"
    blocks[{i}].expert_gate_up_weights[{e}]: fuse(
      "model.layers.{i}.block_sparse_moe.experts.{e}.w1.weight",
      "model.layers.{i}.block_sparse_moe.experts.{e}.w3.weight",
      dim=0
    )
    blocks[{i}].expert_down_weights[{e}]: "model.layers.{i}.block_sparse_moe.experts.{e}.w2.weight"


# =============================================================================
# Qwen3MoE - Qwen 3 MoE Model
# =============================================================================

model Qwen3MoE(
  vocab_size: int = 151936,
  d_model: int = 2048,
  n_layers: int = 28,
  num_heads: int = 16,
  num_kv_heads: int = 4,
  d_ff: int = 1408,
  num_experts: int = 64,
  top_k: int = 8,
  max_seq: int = 40960,
  eps: float = 1e-6,
  rope_theta: float = 1000000.0,
  tie_word_embeddings: bool = true,
  aux_loss_alpha: float = 0.001,
  use_shared_expert: bool = true,
  shared_expert_d_ff: int = 5632
):
  """
  Qwen 3 Mixture-of-Experts model architecture.

  Key features:
  - QK normalization
  - Shared expert (all tokens processed)
  - Many experts with high top-k

  Configurations:
  - Qwen3-MoE-30B-A3B: 30B total, 3B active per token

  Maps to: models/qwen3moe/
  """

  let:
    d_head = d_model // num_heads

    constraint:
      d_model % num_heads == 0, "d_model must be divisible by num_heads"

  params:
    embedding: [vocab_size, d_model]
    blocks: [n_layers] x MoETransformerBlock(
      d_model=d_model,
      num_heads=num_heads,
      num_kv_heads=num_kv_heads,
      d_ff=d_ff,
      num_experts=num_experts,
      top_k=top_k,
      max_seq=max_seq,
      rope_theta=rope_theta,
      eps=eps,
      use_qk_norm=true,              # QK normalization enabled
      activation=swiglu,
      aux_loss_alpha=aux_loss_alpha,
      use_shared_expert=use_shared_expert,
      shared_expert_d_ff=shared_expert_d_ff
    )
    final_norm: [d_model]
    lm_head: tied_to(embedding) if tie_word_embeddings else [vocab_size, d_model]

  forward:
    in: token_ids: [B, T, int32]
    out: (logits: [B, T, vocab_size], aux_loss: float)

    graph:
      token_ids -> embedding(embedding) -> x0
      zeros([B, T, d_model]) -> residual0
      (x0, residual0) -> StackedMoEBlocks(blocks, n_layers) -> (xN, residualN, total_aux_loss)
      (residualN, xN, final_norm) -> fused_residual_rmsnorm(eps=eps) -> (_, xF, _)
      (xF, lm_head) -> matmul(transpose=TN) -> logits

  hf_config:
    architecture: "Qwen3MoeForCausalLM"
    config_class: "Qwen3MoeConfig"

  hf_mapping:
    embedding: "model.embed_tokens.weight"
    final_norm: "model.norm.weight"
    lm_head: "lm_head.weight"

    blocks[{i}].ln1_weight: "model.layers.{i}.input_layernorm.weight"
    blocks[{i}].qkv_weight: fuse(
      "model.layers.{i}.self_attn.q_proj.weight",
      "model.layers.{i}.self_attn.k_proj.weight",
      "model.layers.{i}.self_attn.v_proj.weight",
      dim=0
    )
    blocks[{i}].q_norm_weight: "model.layers.{i}.self_attn.q_norm.weight"
    blocks[{i}].k_norm_weight: "model.layers.{i}.self_attn.k_norm.weight"
    blocks[{i}].out_weight: "model.layers.{i}.self_attn.o_proj.weight"
    blocks[{i}].ln2_weight: "model.layers.{i}.post_attention_layernorm.weight"

    # Router
    blocks[{i}].router_weight: "model.layers.{i}.mlp.gate.weight"

    # Experts
    blocks[{i}].expert_gate_up_weights[{e}]: fuse(
      "model.layers.{i}.mlp.experts.{e}.gate_proj.weight",
      "model.layers.{i}.mlp.experts.{e}.up_proj.weight",
      dim=0
    )
    blocks[{i}].expert_down_weights[{e}]: "model.layers.{i}.mlp.experts.{e}.down_proj.weight"

    # Shared expert
    blocks[{i}].shared_gate_up_weight: fuse(
      "model.layers.{i}.mlp.shared_expert.gate_proj.weight",
      "model.layers.{i}.mlp.shared_expert.up_proj.weight",
      dim=0
    ) if use_shared_expert
    blocks[{i}].shared_down_weight: "model.layers.{i}.mlp.shared_expert.down_proj.weight" if use_shared_expert


# =============================================================================
# Presets
# =============================================================================

preset Mixtral_8x7B:
  """Mixtral-8x7B configuration."""
  vocab_size: 32000
  d_model: 4096
  n_layers: 32
  num_heads: 32
  num_kv_heads: 8
  d_ff: 14336
  num_experts: 8
  top_k: 2
  max_seq: 32768
  rope_theta: 1000000.0

preset Mixtral_8x22B:
  """Mixtral-8x22B configuration."""
  vocab_size: 32000
  d_model: 6144
  n_layers: 56
  num_heads: 48
  num_kv_heads: 8
  d_ff: 16384
  num_experts: 8
  top_k: 2
  max_seq: 65536
  rope_theta: 1000000.0

preset Qwen3_MoE_30B_A3B:
  """Qwen3-MoE-30B-A3B configuration."""
  vocab_size: 151936
  d_model: 2048
  n_layers: 28
  num_heads: 16
  num_kv_heads: 4
  d_ff: 1408
  num_experts: 64
  top_k: 8
  max_seq: 40960
  use_shared_expert: true
  shared_expert_d_ff: 5632
