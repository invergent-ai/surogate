import std.primitives.embedding
import std.primitives.tensor_ops
import std.primitives.matmul
import std.primitives.rmsnorm
import std.blocks.qwen3
import std.modules.stacked_blocks
model Qwen3Model(
  vocab_size: int = 151936,
  d_model: int = 1024,
  n_layers: int = 28,
  num_query_heads: int = 16,
  num_kv_heads: int = 8,
  d_ff: int = 3072,
  max_seq: int = 40960,
  head_size: int = 128,
  eps: float = 1e-6,
  use_qkv_bias: bool = false,
  use_qk_norm: bool = true
):
  """
  Minimal Qwen3 model wiring using Qwen3Block.
  """
  let:
    D = head_size if head_size > 0 else d_model // num_query_heads
  params:
    embedding: [vocab_size, d_model]
    blocks: [n_layers] x Qwen3Block(
      d_model, num_query_heads, num_kv_heads, D, d_ff, max_seq, eps, use_qkv_bias, use_qk_norm
    )
    final_norm: [d_model]
    lm_head: [vocab_size, d_model]
  forward:
    inputs:
      token_ids: [B, T, int32]
      position_ids: [T, int32]
    outputs:
      logits: [B, T, vocab_size]
    graph:
      (token_ids, embedding) -> embedding() -> x0
      zeros(shape=[B, T, d_model], dtype=bf16) -> residual0
      (x0, residual0, position_ids) -> StackedBlocks(blocks, n_layers) -> (xN, residualN)
      (residualN, xN, final_norm) -> fused_residual_rmsnorm(eps=eps) -> (residual_final, xF, rstd_final)
      xF -> view(shape=[B * T, d_model]) -> xF_flat
      (xF_flat, lm_head) -> matmul(transpose=NT) -> logits_flat
      logits_flat -> view(shape=[B, T, vocab_size]) -> logits

  hf_config:
    architecture: "Qwen3ForCausalLM"
    model_type: "qwen3"
    param_mapping:
      d_model: hidden_size
      n_layers: num_hidden_layers
      num_query_heads: num_attention_heads
      num_kv_heads: num_key_value_heads
      d_ff: intermediate_size
      vocab_size: vocab_size
      max_seq: max_position_embeddings
      head_size: head_dim
      eps: rms_norm_eps
      use_qkv_bias: attention_bias
  hf_mapping:
    embedding: "model.embed_tokens.weight"
    final_norm: "model.norm.weight"
    lm_head: "lm_head.weight"
    blocks[{layer}].ln1_weight: "model.layers.{layer}.input_layernorm.weight"
    blocks[{layer}].ln2_weight: "model.layers.{layer}.post_attention_layernorm.weight"
    blocks[{layer}].qkv_weight: fuse(
      "model.layers.{layer}.self_attn.q_proj.weight",
      "model.layers.{layer}.self_attn.k_proj.weight",
      "model.layers.{layer}.self_attn.v_proj.weight",
      dim=0
    )
    blocks[{layer}].qkv_bias: fuse(
      "model.layers.{layer}.self_attn.q_proj.bias",
      "model.layers.{layer}.self_attn.k_proj.bias",
      "model.layers.{layer}.self_attn.v_proj.bias",
      dim=0
    )
    blocks[{layer}].out_weight: "model.layers.{layer}.self_attn.o_proj.weight"
    blocks[{layer}].q_norm_weight: "model.layers.{layer}.self_attn.q_norm.weight"
    blocks[{layer}].k_norm_weight: "model.layers.{layer}.self_attn.k_norm.weight"
    blocks[{layer}].mlp_up_weight: fuse(
      "model.layers.{layer}.mlp.gate_proj.weight",
      "model.layers.{layer}.mlp.up_proj.weight",
      dim=0
    )
    blocks[{layer}].mlp_down_weight: "model.layers.{layer}.mlp.down_proj.weight"
