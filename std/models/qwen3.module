import std.primitives.embedding
import std.primitives.tensor_ops
import std.primitives.matmul
import std.primitives.rmsnorm
import std.blocks.qwen3
import std.modules.stacked_blocks
model Qwen3Model(
  vocab_size: int,
  d_model: int,
  n_layers: int,
  num_query_heads: int,
  num_kv_heads: int,
  d_ff: int,
  max_seq: int,
  head_size: int = 0,
  eps: float = 1e-6,
  use_qkv_bias: bool = false,
  use_qk_norm: bool = true
):
  """
  Minimal Qwen3 model wiring using Qwen3Block.
  """
  let:
    D = head_size if head_size > 0 else d_model // num_query_heads
  params:
    embedding: [vocab_size, d_model]
    blocks: [n_layers] x Qwen3Block(
      d_model, num_query_heads, num_kv_heads, D, d_ff, max_seq, eps, use_qkv_bias, use_qk_norm
    )
    final_norm: [d_model]
    lm_head: [vocab_size, d_model]
  forward:
    inputs:
      token_ids: [B, T, int32]
      position_ids: [T, int32]
    outputs:
      logits: [B, T, vocab_size]
    graph:
      (token_ids, embedding) -> embedding() -> x0
      zeros(shape=[B, T, d_model], dtype=bf16) -> residual0
      (x0, residual0, position_ids) -> StackedBlocks(blocks, n_layers) -> (xN, residualN)
      (residualN, xN, final_norm) -> fused_residual_rmsnorm(eps=eps) -> (residual_final, xF, rstd_final)
      xF -> view(shape=[B * T, d_model]) -> xF_flat
      (xF_flat, lm_head) -> matmul(transpose=NT) -> logits_flat
      logits_flat -> view(shape=[B, T, vocab_size]) -> logits
    save: [token_ids, xF, residual_final, rstd_final]
  backward:
    receives:
      d_logits: [B, T, vocab_size]
    graph:
      d_logits -> view(shape=[B * T, vocab_size]) -> d_logits_flat
      saved.xF -> view(shape=[B * T, d_model]) -> xF_flat
      (d_logits_flat, lm_head) -> matmul(transpose=NN) -> d_xF_flat
      (d_logits_flat, xF_flat) -> matmul(transpose=TN) -> d_lm_head
      d_xF_flat -> view(shape=[B, T, d_model]) -> d_xF
      zeros(shape=[B, T, d_model], dtype=bf16) -> d_residual_next
      (d_xF, d_residual_next, saved.residual_final, final_norm, saved.rstd_final) -> fused_residual_rmsnorm_backward(eps=eps) -> (d_residualN, d_xN, d_final_norm)
      (d_xN, d_residualN) -> StackedBlocksBackward(blocks, n_layers) -> (d_x0, d_residual0)
      (d_x0, saved.token_ids) -> embedding_backward() -> d_embedding

  hf_config:
    architecture: "Qwen3ForCausalLM"
    model_type: "qwen3"
    param_mapping:
      d_model: hidden_size
      n_layers: num_hidden_layers
      num_query_heads: num_attention_heads
      num_kv_heads: num_key_value_heads
      d_ff: intermediate_size
      vocab_size: vocab_size
      max_seq: max_position_embeddings
      head_size: head_dim
      eps: rms_norm_eps
      use_qkv_bias: attention_bias
  hf_mapping:
    embedding: "model.embed_tokens.weight"
    final_norm: "model.norm.weight"
    lm_head: "lm_head.weight"
    blocks[{layer}].ln1_weight: "model.layers.{layer}.input_layernorm.weight"
    blocks[{layer}].ln2_weight: "model.layers.{layer}.post_attention_layernorm.weight"
    blocks[{layer}].qkv_weight: fuse(
      "model.layers.{layer}.self_attn.q_proj.weight",
      "model.layers.{layer}.self_attn.k_proj.weight",
      "model.layers.{layer}.self_attn.v_proj.weight",
      dim=0
    )
    blocks[{layer}].qkv_bias: fuse(
      "model.layers.{layer}.self_attn.q_proj.bias",
      "model.layers.{layer}.self_attn.k_proj.bias",
      "model.layers.{layer}.self_attn.v_proj.bias",
      dim=0
    )
    blocks[{layer}].out_weight: "model.layers.{layer}.self_attn.o_proj.weight"
    blocks[{layer}].q_norm_weight: "model.layers.{layer}.self_attn.q_norm.weight"
    blocks[{layer}].k_norm_weight: "model.layers.{layer}.self_attn.k_norm.weight"
    blocks[{layer}].mlp_up_weight: fuse(
      "model.layers.{layer}.mlp.gate_proj.weight",
      "model.layers.{layer}.mlp.up_proj.weight",
      dim=0
    )
    blocks[{layer}].mlp_down_weight: "model.layers.{layer}.mlp.down_proj.weight"
