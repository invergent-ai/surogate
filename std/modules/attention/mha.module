# Multi-Head Attention Module
# Maps to: csrc/src/modules/primitives/attention.h

"""
Multi-Head Attention (MHA) module.
Standard attention where num_heads == num_kv_heads.
"""

import std.primitives.{flash_attention, rope, matmul}
import std.modules.linear.{QKVProjection, OutputProjection}

# =============================================================================
# MultiHeadAttention - Standard Multi-Head Attention
# =============================================================================

module MultiHeadAttention(
  d_model: int,
  num_heads: int,
  max_seq: int,
  rope_theta: float = 10000.0,
  use_qk_norm: bool = false,
  eps: float = 1e-6
):
  """
  Standard Multi-Head Self-Attention.

  Architecture:
    Q, K, V = QKV_projection(x)
    Q, K = RoPE(Q, K)
    out = FlashAttention(Q, K, V)
    out = output_projection(out)

  Uses FlashAttention for memory-efficient computation.

  Maps to: modules/primitives/AttentionModule (num_heads == num_kv_heads)
  """

  let:
    d_head = d_model // num_heads

    constraint:
      d_model % num_heads == 0, "d_model must be divisible by num_heads"

  params:
    qkv_weight: [3 * d_model, d_model]
    out_weight: [d_model, d_model]
    q_norm_weight: [d_head] if use_qk_norm
    k_norm_weight: [d_head] if use_qk_norm

  forward:
    in: [B, T, d_model]
    out: [B, T, d_model]

    graph:
      # QKV projection
      (in, qkv_weight) -> matmul(transpose=TN) -> qkv
      qkv -> split([d_model, d_model, d_model], dim=-1) -> (q_flat, k_flat, v_flat)

      # Reshape to [B, H, T, D]
      q_flat -> view([B, T, num_heads, d_head]) -> view([B, num_heads, T, d_head]) -> q
      k_flat -> view([B, T, num_heads, d_head]) -> view([B, num_heads, T, d_head]) -> k
      v_flat -> view([B, T, num_heads, d_head]) -> view([B, num_heads, T, d_head]) -> v

      # Optional QK normalization + RoPE
      if use_qk_norm:
        (q, k, q_norm_weight, k_norm_weight) -> qkv_qk_norm_rope(
          base=rope_theta, eps=eps
        ) -> (q_rope, k_rope, q_rstd, k_rstd)
      else:
        (q, k) -> rope(base=rope_theta) -> (q_rope, k_rope)

      # Flash Attention
      (q_rope, k_rope, v) -> flash_attention(causal=true) -> (attn_out, lse)

      # Reshape and output projection
      attn_out -> view([B, T, num_heads * d_head]) -> attn_flat
      (attn_flat, out_weight) -> matmul(transpose=TN) -> out

    save: [in, q, k, v, q_rope, k_rope, attn_out, lse]
    save: [q_rstd, k_rstd] if use_qk_norm

  backward:
    d_out: [B, T, d_model]
    d_in: [B, T, d_model]

    graph:
      # Output projection backward
      d_out -> view([B, T, num_heads * d_head]) -> d_attn_flat
      (d_attn_flat, out_weight) -> matmul(transpose=NN) -> d_attn_out_flat
      d_attn_out_flat -> view([B, num_heads, T, d_head]) -> d_attn_out

      (saved.attn_flat, d_out) -> matmul(transpose=TN, accumulate=true) -> d_out_weight

      # Flash attention backward
      (d_attn_out, saved.q_rope, saved.k_rope, saved.v, saved.attn_out, saved.lse)
        -> flash_attention_backward() -> (d_q_rope, d_k_rope, d_v)

      # RoPE backward (inverse rotation)
      if use_qk_norm:
        (d_q_rope, d_k_rope, saved.q, saved.k, q_norm_weight, k_norm_weight,
         saved.q_rstd, saved.k_rstd)
          -> qkv_qk_norm_rope_backward() -> (d_q, d_k, d_q_norm_weight, d_k_norm_weight)
      else:
        (d_q_rope, d_k_rope) -> rope_backward(base=rope_theta) -> (d_q, d_k)

      # Reshape gradients
      d_q -> view([B, T, d_model]) -> d_q_flat
      d_k -> view([B, T, d_model]) -> d_k_flat
      d_v -> view([B, T, d_model]) -> d_v_flat

      # Concat QKV gradients
      (d_q_flat, d_k_flat, d_v_flat) -> concat(dim=-1) -> d_qkv

      # QKV projection backward
      (d_qkv, qkv_weight) -> matmul(transpose=NN) -> d_in
      (saved.in, d_qkv) -> matmul(transpose=TN, accumulate=true) -> d_qkv_weight
