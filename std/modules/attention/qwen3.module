import std.primitives.matmul
import std.primitives.rope
import std.primitives.attention
import std.primitives.bias
module Qwen3Attention(
  d_model: int,
  num_query_heads: int,
  num_kv_heads: int,
  head_size: int,
  max_seq: int,
  use_qkv_bias: bool = false,
  use_qk_norm: bool = true
):
  """
  Qwen3 attention: GQA + QK-Norm + RoPE + FlashAttention.
  """
  let:
    C = d_model
    Hq = num_query_heads
    Hkv = num_kv_heads
    D = head_size
    QKV = (Hq + 2 * Hkv) * D
    AttnDim = Hq * D
  params:
    qkv_weight: [QKV, C]
    qkv_bias: [QKV] if use_qkv_bias
    out_weight: [C, AttnDim]
    q_norm_weight: [D] if use_qk_norm
    k_norm_weight: [D] if use_qk_norm
    rope_freqs: [max_seq, D // 2, 2, fp32] @frozen
  forward:
    inputs:
      x: [B, T, C]
      position_ids: [T, int32]
    outputs:
      out: [B, T, C]
    graph:
      x -> view(shape=[B * T, C]) -> x_flat
      (x_flat, qkv_weight) -> matmul(transpose=NT) -> qkv_flat
      if use_qkv_bias:
        qkv_flat -> view(shape=[B, T, QKV]) -> qkv_tmp
        (qkv_tmp, qkv_bias) -> bias_add() -> qkv_biased
        qkv_biased -> view(shape=[B, T, Hq + 2 * Hkv, D]) -> qkv_packed
      else:
        qkv_flat -> view(shape=[B, T, Hq + 2 * Hkv, D]) -> qkv_packed
      if use_qk_norm:
        (qkv_packed, q_norm_weight, k_norm_weight, rope_freqs, position_ids) -> qkv_qk_norm_rope() -> (qkv_rope, _, _)
      else:
        (qkv_packed, rope_freqs, position_ids) -> rope(rotary_dim=D) -> qkv_rope
      qkv_rope -> flash_attention(causal=true) -> (att, _)
      att -> view(shape=[B * T, AttnDim]) -> att_flat
      (att_flat, out_weight) -> matmul(transpose=NT) -> out_flat
      out_flat -> view(shape=[B, T, C]) -> out
