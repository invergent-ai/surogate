# Standard Library Modules Index
# Version: 0.1.0

"""
Index of all composite modules in the standard library.

Modules are reusable building blocks that compose primitives
into higher-level neural network components.
"""

# =============================================================================
# Linear Projections (linear.module)
# =============================================================================

export module Linear              # Basic linear projection
export module FusedLinear         # Linear with strided output
export module QKVProjection       # Fused Q/K/V projection
export module GateUpProjection    # Fused gate/up projection (SwiGLU)
export module OutputProjection    # Attention output projection

# =============================================================================
# Normalization (rmsnorm.module)
# =============================================================================

export module RMSNorm             # Root Mean Square Normalization
export module FusedResidualRMSNorm # Fused residual + RMSNorm
export module RMSNormParams       # Parameter-only structure

# =============================================================================
# Feed-Forward Networks (mlp.module)
# =============================================================================

export module SwiGLU_MLP          # SwiGLU FFN (LLaMA, Qwen)
export module GeGLU_MLP           # GeGLU FFN (PaLM)
export module SiLU_MLP            # Non-gated SiLU FFN
export module ReLU2_MLP           # Squared ReLU FFN (Nemotron)
export module GELU_MLP            # Standard GELU FFN (BERT, GPT-2)
export module ExpertMLP           # Single expert FFN (MoE)

# =============================================================================
# Attention (attention/*.module)
# =============================================================================

export module MultiHeadAttention        # Standard MHA (attention/mha.module)
export module GroupedQueryAttention     # GQA (attention/gqa.module)
export module CausalSelfAttention       # Alias for GQA
export module MultiQueryAttention       # MQA (attention/mqa.module)

# =============================================================================
# Mixture-of-Experts (moe/*.module)
# =============================================================================

# Routers
export module SwitchRouter        # Top-K routing (moe/router.module)
export module ExpertChoiceRouter  # Expert-choice routing
export module SigmoidRouter       # Sigmoid gating (DeepSeek)

# Expert computation
export module MoEExperts          # Grouped expert computation (moe/expert.module)
export module MoELayer            # Complete MoE layer
export module SharedExpert        # Shared expert (Nemotron/DeepSeek)
