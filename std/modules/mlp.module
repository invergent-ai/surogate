# MLP Modules
# Maps to: csrc/src/modules/primitives/mlp.h

"""
Feed-forward network (MLP) modules for transformer architectures.
Includes SwiGLU, GeGLU, and standard MLP variants.
"""

import std.primitives.{matmul, swiglu, geglu, silu, relu, relu2, gelu}
import std.modules.linear.{Linear, GateUpProjection}

# =============================================================================
# SwiGLU_MLP - SwiGLU Feed-Forward Network
# =============================================================================

module SwiGLU_MLP(d_model: int, d_ff: int):
  """
  SwiGLU feed-forward network (LLaMA, Qwen, Mistral style).

  Architecture:
    gate_up = x @ gate_up_weight^T     # [B, T, 2*d_ff]
    gate, up = split(gate_up)           # Each [B, T, d_ff]
    hidden = swiglu(gate, up)           # [B, T, d_ff]
    out = hidden @ down_weight^T        # [B, T, d_model]

  Uses activation checkpointing: saves input, recomputes intermediates.

  Maps to: modules/primitives/MLPModule<SwiGLU>
  """

  params:
    gate_up_weight: [2 * d_ff, d_model]
    down_weight: [d_model, d_ff]

  forward:
    in: [B, T, d_model]
    out: [B, T, d_model]

    graph:
      # Up projection (fused gate + up)
      (in, gate_up_weight) -> matmul(transpose=TN) -> gate_up

      # Split and activation
      gate_up -> split([d_ff, d_ff], dim=-1) -> (gate, up)
      (gate, up) -> swiglu() -> hidden

      # Down projection
      (hidden, down_weight) -> matmul(transpose=TN) -> out

    save: [in]
    recompute: [gate_up, gate, up, hidden]

  backward:
    d_out: [B, T, d_model]
    d_in: [B, T, d_model]

    graph:
      # Recompute forward activations
      recompute:
        (saved.in, gate_up_weight) -> matmul(transpose=TN) -> gate_up
        gate_up -> split([d_ff, d_ff], dim=-1) -> (gate, up)
        (gate, up) -> swiglu() -> hidden

      # Down projection backward
      (d_out, down_weight) -> matmul(transpose=NN) -> d_hidden
      (hidden, d_out) -> matmul(transpose=TN, accumulate=true) -> d_down_weight

      # SwiGLU backward
      (d_hidden, gate, up) -> swiglu_backward() -> (d_gate, d_up)

      # Concat gradients
      (d_gate, d_up) -> concat(dim=-1) -> d_gate_up

      # Up projection backward
      (d_gate_up, gate_up_weight) -> matmul(transpose=NN) -> d_in
      (saved.in, d_gate_up) -> matmul(transpose=TN, accumulate=true) -> d_gate_up_weight


# =============================================================================
# GeGLU_MLP - GeGLU Feed-Forward Network
# =============================================================================

module GeGLU_MLP(d_model: int, d_ff: int):
  """
  GeGLU feed-forward network (PaLM style).

  Same architecture as SwiGLU_MLP but uses GELU for gating.

  Maps to: modules/primitives/MLPModule<GeGLU>
  """

  params:
    gate_up_weight: [2 * d_ff, d_model]
    down_weight: [d_model, d_ff]

  forward:
    in: [B, T, d_model]
    out: [B, T, d_model]

    graph:
      (in, gate_up_weight) -> matmul(transpose=TN) -> gate_up
      gate_up -> split([d_ff, d_ff], dim=-1) -> (gate, up)
      (gate, up) -> geglu() -> hidden
      (hidden, down_weight) -> matmul(transpose=TN) -> out

    save: [in]
    recompute: [gate_up, gate, up, hidden]

  backward:
    # Same structure as SwiGLU_MLP with geglu_backward


# =============================================================================
# SiLU_MLP - Non-Gated SiLU MLP
# =============================================================================

module SiLU_MLP(d_model: int, d_ff: int):
  """
  Simple SiLU MLP without gating (non-GLU variant).

  Architecture:
    hidden = silu(x @ up_weight^T)
    out = hidden @ down_weight^T

  Maps to: modules/primitives/MLPModule<SiLU>
  """

  params:
    up_weight: [d_ff, d_model]      # Note: d_ff, not 2*d_ff
    down_weight: [d_model, d_ff]

  forward:
    in: [B, T, d_model]
    out: [B, T, d_model]

    graph:
      (in, up_weight) -> matmul(transpose=TN) -> hidden_pre
      hidden_pre -> silu() -> hidden
      (hidden, down_weight) -> matmul(transpose=TN) -> out

    save: [in]
    recompute: [hidden_pre, hidden]

  backward:
    d_out: [B, T, d_model]
    d_in: [B, T, d_model]

    graph:
      recompute:
        (saved.in, up_weight) -> matmul(transpose=TN) -> hidden_pre
        hidden_pre -> silu() -> hidden

      (d_out, down_weight) -> matmul(transpose=NN) -> d_hidden
      (hidden, d_out) -> matmul(transpose=TN, accumulate=true) -> d_down_weight

      (d_hidden, hidden_pre) -> silu_backward() -> d_hidden_pre

      (d_hidden_pre, up_weight) -> matmul(transpose=NN) -> d_in
      (saved.in, d_hidden_pre) -> matmul(transpose=TN, accumulate=true) -> d_up_weight


# =============================================================================
# ReLU2_MLP - Squared ReLU MLP (Nemotron)
# =============================================================================

module ReLU2_MLP(d_model: int, d_ff: int):
  """
  ReLUÂ² MLP (Nemotron-H style).

  Uses squared ReLU activation for stronger gradient signal.

  Maps to: modules/primitives/MLPModule<ReLU2>
  """

  params:
    up_weight: [d_ff, d_model]
    down_weight: [d_model, d_ff]

  forward:
    in: [B, T, d_model]
    out: [B, T, d_model]

    graph:
      (in, up_weight) -> matmul(transpose=TN) -> hidden_pre
      hidden_pre -> relu2() -> hidden
      (hidden, down_weight) -> matmul(transpose=TN) -> out

    save: [in]
    recompute: [hidden_pre, hidden]

  backward:
    # Similar structure with relu2_backward


# =============================================================================
# GELU_MLP - Standard GELU MLP (BERT/GPT-2)
# =============================================================================

module GELU_MLP(d_model: int, d_ff: int, bias: bool = true):
  """
  Standard GELU MLP (BERT, GPT-2 style).

  Architecture:
    hidden = gelu(x @ up_weight^T + up_bias)
    out = hidden @ down_weight^T + down_bias

  Maps to: modules/primitives/MLPModule<GELU>
  """

  params:
    up_weight: [d_ff, d_model]
    up_bias: [d_ff] if bias
    down_weight: [d_model, d_ff]
    down_bias: [d_model] if bias

  forward:
    in: [B, T, d_model]
    out: [B, T, d_model]

    graph:
      (in, up_weight) -> matmul(transpose=TN) -> hidden_pre
      if bias:
        (hidden_pre, up_bias) -> bias_add() -> hidden_pre_biased
        hidden_pre_biased -> gelu() -> hidden
      else:
        hidden_pre -> gelu() -> hidden

      (hidden, down_weight) -> matmul(transpose=TN) -> out_pre
      if bias:
        (out_pre, down_bias) -> bias_add() -> out
      else:
        out_pre -> out

    save: [in]
    recompute: [hidden_pre, hidden]


# =============================================================================
# ExpertMLP - Single Expert MLP (for MoE)
# =============================================================================

module ExpertMLP(d_model: int, d_ff: int, activation: enum = swiglu):
  """
  Single expert MLP for Mixture-of-Experts architectures.

  Same structure as standard MLP but instantiated per-expert.
  Typically uses SwiGLU activation.

  Maps to: modules/moe/Expert
  """

  params:
    gate_up_weight: [2 * d_ff, d_model] if activation in [swiglu, geglu]
    up_weight: [d_ff, d_model] if activation not in [swiglu, geglu]
    down_weight: [d_model, d_ff]

  forward:
    in: [N_tokens, d_model]        # Variable number of tokens per expert
    out: [N_tokens, d_model]

    graph:
      if activation == swiglu:
        (in, gate_up_weight) -> matmul(transpose=TN) -> gate_up
        gate_up -> split([d_ff, d_ff], dim=-1) -> (gate, up)
        (gate, up) -> swiglu() -> hidden
      else if activation == geglu:
        (in, gate_up_weight) -> matmul(transpose=TN) -> gate_up
        gate_up -> split([d_ff, d_ff], dim=-1) -> (gate, up)
        (gate, up) -> geglu() -> hidden
      else if activation == silu:
        (in, up_weight) -> matmul(transpose=TN) -> hidden_pre
        hidden_pre -> silu() -> hidden
      else if activation == relu2:
        (in, up_weight) -> matmul(transpose=TN) -> hidden_pre
        hidden_pre -> relu2() -> hidden

      (hidden, down_weight) -> matmul(transpose=TN) -> out

    save: [in]
    recompute: [gate_up, gate, up, hidden] if activation in [swiglu, geglu]
    recompute: [hidden_pre, hidden] if activation not in [swiglu, geglu]
