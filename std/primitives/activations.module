# Activation Primitives
# Native implementations: csrc/src/kernels/swiglu.cu, activation.cu

primitive swiglu:
  """
  SwiGLU activation on concatenated gate/up tensor.
  Input x is [up, gate] concatenated on the last dimension.
  """

  forward:
    in: x: [*, 2 * D]
    out: [*, D]
    _

  backward:
    in: (d_out: [*, D], x: [*, 2 * D])
    out: d_x: [*, 2 * D]
    _

  save: [x]

  impl:
    forward: kernels.swiglu_forward
    backward: kernels.swiglu_backward


primitive silu:
  """
  SiLU (Swish) activation.
  """

  forward:
    in: x: [*]
    out: [*]
    _

  backward:
    in: (d_out: [*], x: [*])
    out: d_x: [*]
    _

  save: [x]

  impl:
    forward: kernels.silu_forward
    backward: kernels.silu_backward


primitive relu2:
  """
  Squared ReLU activation.
  """

  forward:
    in: x: [*]
    out: [*]
    _

  backward:
    in: (d_out: [*], x: [*])
    out: d_x: [*]
    _

  save: [x]

  impl:
    forward: kernels.relu2_forward
    backward: kernels.relu2_backward


primitive silu_mul:
  """
  Elementwise SiLU(gate) * up.
  Used by fast LoRA kernels.
  """

  forward:
    in: (gate: [*, D], up: [*, D])
    out: [*, D]
    _

  backward:
    in: (d_out: [*, D], gate: [*, D], up: [*, D])
    out: (d_gate: [*, D], d_up: [*, D])
    _

  save: [gate, up]

  impl:
    forward: kernels.silu_mul_forward
    backward: kernels.silu_mul_backward_inplace
