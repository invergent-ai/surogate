# Attention Primitives
# Native implementations: csrc/src/kernels/attention.cu, csrc/src/kernels/cudnn_att.cpp

primitive flash_attention:
  """
  Scaled dot-product attention using cuDNN SDPA backend.

  Expects packed QKV input: [B, T, Hq + 2*Hkv, D].
  Returns log-sum-exp (lse) for numerically stable backward.
  """

  params:
    causal: bool = true
    softmax_scale: float? = None
    window_size: int? = None

  forward:
    in: qkv: [B, T, Hq + 2 * Hkv, D]
    out: (
      out: [B, T, Hq, D],
      lse: [B, Hq, T]
    )
    _

  backward:
    in: (
      d_out: [B, T, Hq, D],
      out: [B, T, Hq, D],
      lse: [B, Hq, T],
      qkv: [B, T, Hq + 2 * Hkv, D]
    )
    out: d_qkv: [B, T, Hq + 2 * Hkv, D]
    _

  save: [qkv, out, lse]

  impl:
    forward: kernels.attention_forward_cudnn
    backward: kernels.attention_backward_cudnn
