# Mixture-of-Experts (MoE) Primitives
# Native implementations: csrc/src/kernels/moe_kernels.cu

primitive moe_softmax:
  """
  Row-wise softmax for MoE routing logits.
  """

  forward:
    in: logits: [BT, E]
    out: probs: [BT, E]
    _

  backward:
    in: (d_probs: [BT, E], probs: [BT, E])
    out: d_logits: [BT, E]
    _

  save: [probs]

  impl:
    forward: kernels.moe_softmax_forward
    backward: kernels.moe_softmax_backward


primitive moe_sigmoid:
  """
  Element-wise sigmoid for routing (DeepSeek-style).
  """

  forward:
    in: logits: [*]
    out: scores: [*]
    _

  backward:
    in: (d_scores: [*], scores: [*])
    out: d_logits: [*]
    _

  save: [scores]

  impl:
    forward: kernels.moe_sigmoid_forward
    backward: kernels.moe_sigmoid_backward


primitive moe_scale:
  """
  Scale routing scores by a scalar.
  """

  params:
    scale: float

  forward:
    in: x: [*]
    out: [*]
    _

  impl:
    forward: kernels.moe_scale_forward


primitive moe_topk:
  """
  Top-K expert selection.
  """

  params:
    top_k: int
    normalize: bool = true

  forward:
    in: probs: [BT, E]
    out: (
      weights: [BT, top_k],
      indices: [BT, top_k, int32]
    )
    _

  backward:
    in: (
      d_weights: [BT, top_k],
      probs: [BT, E],
      indices: [BT, top_k, int32]
    )
    out: d_probs: [BT, E]
    _

  save: [probs, indices]

  impl:
    forward: kernels.moe_topk_forward
    backward: kernels.moe_topk_backward


primitive moe_compute_expert_counts:
  """
  Histogram of tokens per expert.
  """

  params:
    num_experts: int
    top_k: int

  forward:
    in: indices: [BT, top_k, int32]
    out: expert_counts: [num_experts, int32]
    _

  impl:
    forward: kernels.moe_compute_expert_counts


primitive moe_compute_expert_offsets:
  """
  Exclusive prefix sum over expert counts.
  """

  params:
    num_experts: int

  forward:
    in: expert_counts: [num_experts, int32]
    out: expert_offsets: [num_experts + 1, int32]
    _

  impl:
    forward: kernels.moe_compute_expert_offsets


primitive moe_build_indices:
  """
  Build gather/scatter indices for token permutation.
  """

  params:
    num_experts: int
    top_k: int

  forward:
    in: (
      indices: [BT, top_k, int32],
      expert_offsets: [num_experts + 1, int32],
      expert_positions: [num_experts, int32]
    )
    out: (
      gather_indices: [BT * top_k, int32],
      scatter_indices: [BT * top_k, int32]
    )
    _

  impl:
    forward: kernels.moe_build_indices


primitive moe_remap_expert_indices:
  """
  Remap expert indices to a compact range.
  """

  params:
    top_k: int

  forward:
    in: (
      indices: [BT, top_k, int32],
      expert_to_compact: [E_total, int32]
    )
    out: remapped_indices: [BT, top_k, int32]
    _

  impl:
    forward: kernels.moe_remap_expert_indices


primitive moe_permute:
  """
  Permute tokens to expert-grouped order.
  """

  params:
    top_k: int

  forward:
    in: (
      x: [BT, C],
      gather_indices: [BT * top_k, int32]
    )
    out: permuted: [BT * top_k, C]
    _

  impl:
    forward: kernels.moe_permute_tokens


primitive moe_unpermute:
  """
  Unpermute and combine expert outputs back to token order.
  """

  params:
    top_k: int

  forward:
    in: (
      expert_out: [BT * top_k, C],
      weights: [BT, top_k],
      scatter_indices: [BT * top_k, int32]
    )
    out: combined: [BT, C]
    _

  impl:
    forward: kernels.moe_unpermute_and_combine


primitive moe_aux_loss:
  """
  Auxiliary load-balancing loss for MoE routing.
  """

  params:
    num_experts: int
    top_k: int
    aux_loss_coef: float = 0.01

  forward:
    in: (
      probs: [BT, num_experts],
      indices: [BT, top_k, int32]
    )
    out: aux_loss: float
    _

  impl:
    forward: kernels.moe_compute_aux_loss


primitive moe_grouped_gemm:
  """
  Grouped GEMM for non-gated MoE experts.
  """

  params:
    transpose: enum(NN, NT, TN, TT) = TN

  forward:
    in: (
      input: [BT, K],
      weights: [E, M, K],
      offsets: [E + 1, int32]
    )
    out: output: [BT, M]
    _

  impl:
    forward: kernels.moe_grouped_gemm


primitive moe_grouped_gemm_gate_up:
  """
  Grouped GEMM for fused gate+up projection across experts.
  """

  forward:
    in: (
      input: [BT, C],
      weights: [E, 2 * D, C],
      offsets: [E + 1, int32]
    )
    out: gate_up: [BT, 2 * D]
    _

  impl:
    forward: kernels.moe_grouped_gemm_gate_up


primitive moe_grouped_gemm_down:
  """
  Grouped GEMM for expert down projection across experts.
  """

  forward:
    in: (
      input: [BT, D],
      weights: [E, C, D],
      offsets: [E + 1, int32]
    )
    out: output: [BT, C]
    _

  impl:
    forward: kernels.moe_grouped_gemm_down
