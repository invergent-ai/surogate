# Quantization Primitives
# Native implementations: csrc/src/kernels/block_quant.cu, bnb_quant.cu, kernels.cpp

primitive quantize_fp8_e4m3:
  """
  Per-tensor FP8 E4M3 quantization using abs-max scaling.
  """

  forward:
    in: x: [*, bf16]
    out: (
      x_quant: [*, fp8_e4m3],
      scale: float
    )
    _

  impl:
    forward: kernels.quantize_with_abs_max


primitive quantize_fp8_e5m2:
  """
  Per-tensor FP8 E5M2 quantization using abs-max scaling.
  """

  forward:
    in: x: [*, bf16]
    out: (
      x_quant: [*, fp8_e5m2],
      scale: float
    )
    _

  impl:
    forward: kernels.quantize_with_abs_max


primitive quantize_fp8_delayed_e4m3:
  """
  Delayed-scale FP8 E4M3 quantization.
  """

  params:
    amax_history_len: int = 16

  forward:
    in: (
      x: [*, bf16],
      amax_history: [amax_history_len],
      scale: float
    )
    out: (
      x_quant: [*, fp8_e4m3],
      new_amax: float,
      new_scale: float
    )
    _

  impl:
    forward: kernels.quantize_with_delayed_scale


primitive quantize_fp8_delayed_e5m2:
  """
  Delayed-scale FP8 E5M2 quantization.
  """

  params:
    amax_history_len: int = 16

  forward:
    in: (
      x: [*, bf16],
      amax_history: [amax_history_len],
      scale: float
    )
    out: (
      x_quant: [*, fp8_e5m2],
      new_amax: float,
      new_scale: float
    )
    _

  impl:
    forward: kernels.quantize_with_delayed_scale


primitive quantize_fp8_per_block:
  """
  Per-block FP8 E4M3 quantization for weights.
  """

  params:
    block_size: int = 64

  forward:
    in: weight: [*, bf16]
    out: (
      weight_quant: [*, fp8_e4m3],
      block_scales: [num_blocks, fp32]
    )
    _

  impl:
    forward: kernels.quantize_per_block


primitive dequantize_fp8_per_block:
  """
  Dequantize per-block FP8 E4M3 weights.
  """

  forward:
    in: (
      weight_quant: [*, fp8_e4m3],
      block_scales: [num_blocks, fp32]
    )
    out: weight: [*, bf16]
    _

  impl:
    forward: kernels.dequantize_per_block


primitive quantize_fp4:
  """
  FP4 E2M1 quantization with block scaling.
  """

  params:
    block_size: int = 16

  forward:
    in: x: [*, bf16]
    out: (
      x_quant: [*, fp4_e2m1],
      block_scales: [num_blocks, fp8_e4m3],
      global_scale: float
    )
    _

  impl:
    forward: kernels.quantize_fp4_block


primitive quantize_fp4_auto:
  """
  FP4 quantization with automatic scale computation.
  """

  params:
    block_size: int = 16

  forward:
    in: x: [*, bf16]
    out: (
      x_quant: [*, fp4_e2m1],
      block_scales: [num_blocks, fp8_e4m3],
      global_scale: float
    )
    _

  impl:
    forward: kernels.quantize_fp4_block_auto_scale


primitive quantize_fp4_stochastic:
  """
  FP4 quantization with stochastic rounding.
  """

  params:
    block_size: int = 16

  forward:
    in: x: [*, bf16]
    out: (
      x_quant: [*, fp4_e2m1],
      block_scales: [num_blocks, fp8_e4m3],
      global_scale: float
    )
    _

  impl:
    forward: kernels.quantize_fp4_block_stochastic


primitive quantize_fp4_4o6:
  """
  Four-over-six adaptive FP4 quantization.
  """

  params:
    block_size: int = 16

  forward:
    in: x: [*, bf16]
    out: (
      x_quant: [*, fp4_e2m1],
      block_scales: [num_blocks, fp8_e4m3],
      global_scale: float
    )
    _

  impl:
    forward: kernels.quantize_fp4_block_4o6_auto_scale


primitive quantize_fp4_weight:
  """
  FP4 weight quantization for matmul B operand.
  """

  params:
    block_size: int = 16

  forward:
    in: weight: [N, K, bf16]
    out: (
      weight_quant: [N, K, fp4_e2m1],
      block_scales: [num_blocks, fp8_e4m3],
      global_scale: float
    )
    _

  impl:
    forward: kernels.quantize_fp4_weight


primitive quantize_fp4_weight_2d:
  """
  FP4 weight quantization with 2D block scales.
  """

  forward:
    in: weight: [N, K, bf16]
    out: (
      weight_quant: [N, K, fp4_e2m1],
      block_scales_2d: [N // 16, K // 16, fp8_e4m3],
      global_scale: float
    )
    _

  impl:
    forward: kernels.quantize_fp4_weight_2d


primitive dequantize_fp4:
  """
  Dequantize FP4 E2M1 tensor to BF16.
  """

  forward:
    in: (
      x_quant: [*, fp4_e2m1],
      block_scales: [num_blocks, fp8_e4m3]
    )
    out: x: [*, bf16]
    _

  impl:
    forward: kernels.dequantize_fp4_block


primitive fp4_alpha_scale:
  """
  Apply FP4 alpha correction after matmul.
  """

  forward:
    in: (
      matmul_output: [*, fp32],
      amax_a: float,
      amax_b: float
    )
    out: corrected: [*, bf16]
    _

  impl:
    forward: kernels.fp4_alpha_scale


primitive compute_fp4_alpha:
  """
  Compute FP4 alpha correction factor.
  """

  forward:
    in: (
      amax_a: float,
      amax_b: float
    )
    out: alpha: float
    _

  impl:
    forward: kernels.compute_fp4_alpha


primitive hadamard_transform:
  """
  Random Hadamard transform for distribution smoothing.
  """

  forward:
    in: x: [*]
    out: [*]
    _

  backward:
    in: d_out: [*]
    out: d_x: [*]
    _

  impl:
    forward: kernels.hadamard_transform_forward
    backward: kernels.hadamard_transform_inverse
