# Rotary Position Embedding (RoPE) Primitives
# Native implementations: csrc/src/kernels/rope.cu

primitive precompute_rope_freqs:
  """
  Precompute RoPE frequency tensor.
  """

  params:
    dim: int
    end: int
    theta: float = 10000.0

  forward:
    in: ()
    out: freqs: [end, dim // 2, 2]
    _

  impl:
    forward: kernels.precompute_freqs_cis


primitive rope:
  """
  Apply RoPE to packed QKV tensor using precomputed frequencies.
  """

  params:
    rotary_dim: int? = None

  forward:
    in: (
      qkv: [B, T, Hq + 2 * Hkv, D],
      freqs: [S, D // 2, 2],
      position_ids: [T, int32]
    )
    out: qkv_rope: [B, T, Hq + 2 * Hkv, D]
    _

  backward:
    in: (
      d_qkv_rope: [B, T, Hq + 2 * Hkv, D],
      freqs: [S, D // 2, 2],
      position_ids: [T, int32]
    )
    out: d_qkv: [B, T, Hq + 2 * Hkv, D]
    _

  impl:
    forward: kernels.rope_forward
    backward: kernels.rope_backward


primitive rope_fused:
  """
  Apply RoPE with on-the-fly sin/cos computation (no freqs tensor).
  """

  params:
    theta: float = 10000.0

  forward:
    in: (
      qkv: [B, T, Hq + 2 * Hkv, D],
      position_ids: [T, int32]
    )
    out: qkv_rope: [B, T, Hq + 2 * Hkv, D]
    _

  backward:
    in: (
      d_qkv_rope: [B, T, Hq + 2 * Hkv, D],
      position_ids: [T, int32]
    )
    out: d_qkv: [B, T, Hq + 2 * Hkv, D]
    _

  impl:
    forward: kernels.rope_fused_forward
    backward: kernels.rope_fused_backward


primitive qkv_qk_norm_rope:
  """
  Fused per-head Q/K RMSNorm + RoPE on packed QKV tensor.
  """

  params:
    eps: float = 1e-6

  forward:
    in: (
      qkv: [B, T, Hq + 2 * Hkv, D],
      q_weight: [D],
      k_weight: [D],
      freqs: [S, D // 2, 2],
      position_ids: [T, int32]
    )
    out: (
      qkv_rope: [B, T, Hq + 2 * Hkv, D],
      q_rstd: [B, Hq, T],
      k_rstd: [B, Hkv, T]
    )
    _

  impl:
    forward: kernels.qkv_qk_norm_rope_forward
