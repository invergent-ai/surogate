import os
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING, Callable, Optional, Protocol, runtime_checkable

import torch
import yaml

from surogate.grpo.utils.pathing import get_stable_ckpt_steps
from surogate.utils.logger import get_logger

if TYPE_CHECKING:
    from surogate.core.config.grpo_orch_config import OrchestratorConfig


logger = get_logger()


# ---------------------------------------------------------------------------
# Local replacements for prime_rl.trainer utilities
# ---------------------------------------------------------------------------


@dataclass
class LoRAConfig:
    """Minimal LoRA configuration for multi-run validation."""
    rank: int = 16
    alpha: float = 32.0


class World:
    """Distributed training topology info (parsed from torchrun env vars)."""

    def __init__(self):
        self.rank = int(os.environ.get("RANK", "0"))
        self.world_size = int(os.environ.get("WORLD_SIZE", "1"))
        self.local_rank = int(os.environ.get("LOCAL_RANK", "0"))
        self.local_world_size = int(os.environ.get("LOCAL_WORLD_SIZE", "1"))
        self.num_nodes = self.world_size // max(self.local_world_size, 1)

    @property
    def is_master(self) -> bool:
        return self.rank == 0


_WORLD: World | None = None


def get_world() -> World:
    """Returns the singleton World instance (lazy-initialized)."""
    global _WORLD
    if _WORLD is None:
        _WORLD = World()
    return _WORLD


# LoRA global state (replaces prime_rl.trainer.models.layers.lora globals)

_LORA_NUM_TOKENS: torch.Tensor | None = None
_SCALING_FACTORS: torch.Tensor | None = None


def set_lora_num_tokens(num_tokens: torch.Tensor | None, reset_reference: bool = False) -> None:
    global _LORA_NUM_TOKENS
    if _LORA_NUM_TOKENS is None or reset_reference:
        _LORA_NUM_TOKENS = num_tokens
    elif num_tokens is not None:
        _LORA_NUM_TOKENS.copy_(num_tokens)


def get_lora_num_tokens() -> torch.Tensor:
    assert _LORA_NUM_TOKENS is not None, "LoRA num_tokens not initialized"
    return _LORA_NUM_TOKENS


def set_multilora_scaling(scaling_factors: torch.Tensor | None, reset_reference: bool = False) -> None:
    global _SCALING_FACTORS
    if _SCALING_FACTORS is None or reset_reference:
        _SCALING_FACTORS = scaling_factors
    elif scaling_factors is not None:
        _SCALING_FACTORS.copy_(scaling_factors)


def get_multilora_scaling() -> torch.Tensor:
    assert _SCALING_FACTORS is not None, "Scaling factors not initialized"
    return _SCALING_FACTORS


@runtime_checkable
class MultiLoRAModule(Protocol):
    """Protocol for multi-adapter LoRA modules registered with MultiRunManager."""

    def named_parameters_for_adapter(self, idx: int) -> list[tuple[str, torch.nn.Parameter]]: ...
    def reset_parameters(self, idx: int | None = None) -> None: ...


# ---------------------------------------------------------------------------
# Run progress tracking
# ---------------------------------------------------------------------------


@dataclass
class Progress:
    step: int = 0
    total_tokens: int = 0
    total_samples: int = 0


# ---------------------------------------------------------------------------
# MultiRunManager
# ---------------------------------------------------------------------------


class MultiRunManager:
    """This class stores information about the runs in the system."""

    # =========================================================================
    # Initialization
    # =========================================================================

    def __init__(self, output_dir: Path, max_runs: int, device: torch.device = torch.device("cpu")):
        self.output_dir = output_dir
        self.max_runs = max_runs

        self.idx_2_id: dict[int, str] = {}
        self.id_2_idx: dict[str, int] = {}
        self.unused_idxs = {i for i in range(self.max_runs)}

        self.progress: dict[int, Progress] = {}
        self.config: dict[int, "OrchestratorConfig"] = {}
        self.ready_to_update = [False] * max_runs

        self._creation_hooks: list[Callable[[int, str], None]] = []
        self._deletion_hooks: list[Callable[[int, str], None]] = []
        self._discovered_hooks: list[Callable[[int, str, "OrchestratorConfig"], None]] = []
        self._forgotten_hooks: list[Callable[[int, str], None]] = []
        self._config_validation_hooks: list[Callable[["OrchestratorConfig"], tuple[bool, str]]] = []

        self.world = get_world()

        # Store modules with their FQN prefixes for parameter management
        self._modules: list[tuple[str, MultiLoRAModule]] = []

        # Initialize lora globals on device
        set_lora_num_tokens(None, reset_reference=True)
        set_multilora_scaling(None, reset_reference=True)

        set_lora_num_tokens(torch.zeros(max_runs, dtype=torch.int32, device=device), reset_reference=True)
        self.lora_num_tokens = get_lora_num_tokens()

        set_multilora_scaling(
            torch.ones(max_runs, dtype=torch.bfloat16, device=device) * 1000_000.0, reset_reference=True
        )
        self.scaling_factors = get_multilora_scaling()

    # =========================================================================
    # Hook Registration
    # =========================================================================

    def register_creation_hook(self, hook: Callable[[int, str], None]) -> None:
        """Register a hook to be called when a new run is created.

        Args:
            hook: A callable that takes (idx: int, run_id: str) as arguments.
                  Called on all ranks when a new run is added to the system.
        """
        self._creation_hooks.append(hook)

    def register_deletion_hook(self, hook: Callable[[int, str], None]) -> None:
        """Register a hook to be called when a run is deleted.

        Args:
            hook: A callable that takes (idx: int, run_id: str) as arguments.
                  Called on all ranks when a run is removed from the system.
        """
        self._deletion_hooks.append(hook)

    def register_discovered_hook(self, hook: Callable[[int, str, "OrchestratorConfig"], None]) -> None:
        """Register a hook to be called when a new run is discovered (master only).

        Args:
            hook: A callable that takes (idx: int, run_id: str, config: OrchestratorConfig).
                  Called only on master rank in discover_runs() when a new run is found.
        """
        if not self.world.is_master:
            raise RuntimeError("register_discovered_hook() must only be called on the master rank")
        self._discovered_hooks.append(hook)

    def register_forgotten_hook(self, hook: Callable[[int, str], None]) -> None:
        """Register a hook to be called when a run is forgotten/removed (master only).

        Args:
            hook: A callable that takes (idx: int, run_id: str).
                  Called only on master rank in discover_runs() when a run is removed.
        """
        if not self.world.is_master:
            raise RuntimeError("register_forgotten_hook() must only be called on the master rank")
        self._forgotten_hooks.append(hook)

    def register_config_validation_hook(self, hook: Callable[["OrchestratorConfig"], tuple[bool, str]]) -> None:
        """Register a hook to validate orchestrator config when a run is created.

        Args:
            hook: A callable that takes (config: OrchestratorConfig) and returns
                  (is_valid: bool, error_message: str). Error message is used when invalid.
        """
        if not self.world.is_master:
            raise RuntimeError("register_config_validation_hook() must only be called on the master rank")
        self._config_validation_hooks.append(hook)

    # =========================================================================
    # Module Registration and Parameter Management
    # =========================================================================

    def register_module(self, prefix: str, module: MultiLoRAModule) -> None:
        """Register a MultiLoRA module with its FQN prefix.

        This allows MultiRunManager to manage parameter access, reset, and state dict slicing
        for multi-adapter LoRA modules.

        Args:
            prefix: The module's fully qualified name in the model
                   (e.g., "model.layers.0.self_attn.q_proj")
            module: The MultiLoRA module to register
        """
        self._modules.append((prefix, module))

    def get_named_parameters_for_run(self, idx: int) -> list[tuple[str, torch.nn.Parameter]]:
        """Get named parameters for a specific run index.

        Args:
            idx: The run index to get parameters for

        Returns:
            List of (name, parameter) tuples for the specified run index
        """
        params = []
        for prefix, module in self._modules:
            for name, param in module.named_parameters_for_adapter(idx):
                params.append((f"{prefix}.{name}.weight", param))
        return params

    def get_state_dict_for_run(self, idx: int) -> dict[str, torch.Tensor]:
        """Get state dict for a specific run index.

        Args:
            idx: The run index to get state dict for

        Returns:
            State dict for the specified run index
        """
        state_dict = {}
        for prefix, module in self._modules:
            # Check if module has a custom state_dict_for_adapter method (e.g., MoE modules)
            # which returns vLLM-compatible per-expert format
            if hasattr(module, "state_dict_for_adapter"):
                for name, tensor in module.state_dict_for_adapter(idx).items():
                    state_dict[f"{prefix}.{name}"] = tensor.detach()
            else:
                # Default: use named_parameters_for_adapter
                for name, param in module.named_parameters_for_adapter(idx):
                    state_dict[f"{prefix}.{name}.weight"] = param.detach()
        return state_dict

    def reset_run_parameters(self, idx: int) -> None:
        """Reset parameters for a specific run index.

        Called when a new run is created to initialize fresh adapter weights.

        Args:
            idx: The run index to reset parameters for
        """
        for _, module in self._modules:
            module.reset_parameters(idx)

    # =========================================================================
    # Config Loading
    # =========================================================================

    def get_orchestrator_config(self, run_id: str) -> Optional["GRPOOrchestratorConfig"]:
        """Load and validate orchestrator config for a run.

        Returns None if config doesn't exist, fails to parse, or fails validation.
        Writes error to config dir for orchestrator to consume.
        """
        config_path = self.output_dir / run_id / "control" / "orch.yaml"
        config_dir = config_path.parent
        error_path = config_dir / "config_validation_error.txt"

        if not config_path.exists():
            logger.error(f"Run {run_id}: No orchestrator config found at {config_path}")
            return None

        try:
            with open(config_path, "r") as f:
                config_dict = yaml.safe_load(f)

            from surogate.core.config.grpo_orch_config import GRPOOrchestratorConfig
            from surogate.utils.dict import DictDefault

            config = GRPOOrchestratorConfig(DictDefault(config_dict))
        except Exception as e:
            if error_path.parent.exists():
                with open(error_path, "w") as f:
                    f.write(f"Error parsing orchestrator config:\n{str(e)}\n")
            logger.error(f"Run {run_id}: Error parsing orchestrator config: {e}")
            return None

        # Run registered validation hooks
        for hook in self._config_validation_hooks:
            is_valid, error_message = hook(config)
            if not is_valid:
                logger.error(f"Run {run_id}: {error_message}")
                if error_path.parent.exists():
                    with open(error_path, "w") as f:
                        f.write(f"{error_message}\n")
                return None

        # Config is valid, remove any stale error file
        if error_path.exists():
            error_path.unlink()

        return config

    # =========================================================================
    # Internal Run Data Helpers
    # =========================================================================

    def _create_run_data(self, new_run: str, new_id: int, config: "OrchestratorConfig") -> None:
        """Update data structures for a new run (no hooks or param reset)."""
        self.id_2_idx[new_run] = new_id
        self.unused_idxs.remove(new_id)
        self.idx_2_id[new_id] = new_run

        # Set progress based on resume_step config (match orchestrator behavior)
        self.progress[new_id] = Progress()
        if config.ckpt is None or config.ckpt.resume_step is None:
            self.progress[new_id].step = 0
        elif config.ckpt.resume_step == -1:
            stable_steps = get_stable_ckpt_steps(self.get_run_dir(new_id) / "checkpoints")
            self.progress[new_id].step = max(stable_steps) if stable_steps else 0
        else:
            self.progress[new_id].step = config.ckpt.resume_step

        # Store the parsed config
        self.config[new_id] = config

    def _delete_run_data(self, deleted_run: str, deleted_idx: int) -> None:
        """Update data structures for a deleted run (internal cleanup only, no hooks)."""
        del self.progress[deleted_idx]
        if deleted_idx in self.config:
            del self.config[deleted_idx]

        # A big value might help make it error loudly
        self.scaling_factors[deleted_idx] = 1000_000.0

        # Process mappings
        self.unused_idxs.add(deleted_idx)
        del self.idx_2_id[deleted_idx]
        del self.id_2_idx[deleted_run]

    def _create_run_hooks(self, new_id: int, new_run: str) -> None:
        """Reset parameters and call creation hooks for a run."""
        self.reset_run_parameters(new_id)
        for hook in self._creation_hooks:
            hook(new_id, new_run)

    def _delete_run_hooks(self, deleted_idx: int, deleted_run: str) -> None:
        """Call deletion hooks for a run."""
        for hook in self._deletion_hooks:
            hook(deleted_idx, deleted_run)

    # =========================================================================
    # Run Discovery, Synchronization, and Eviction
    # =========================================================================

    def evict_run(self, idx: int, reason: str) -> None:
        """Evict a run by writing the reason to a file for the orchestrator to read.

        The orchestrator will error and surface this reason. The run will be
        ignored by future discover_runs() calls.
        Note that the run is not deleted on master until the next discover_runs() call.
        And not deleted on other ranks until the next synchronize_state() call.

        Args:
            idx: The run index to evict
            reason: The reason for eviction (will be shown to user)
        """
        if not self.world.is_master:
            raise RuntimeError("evict_run() must only be called on the master rank")

        if idx not in self.idx_2_id:
            logger.warning(f"Run index {idx} not found, cannot evict")
            return

        run_id = self.idx_2_id[idx]
        run_dir = self.output_dir / run_id
        config_dir = run_dir / "control"
        config_dir.mkdir(parents=True, exist_ok=True)

        evicted_path = config_dir / "evicted.txt"
        with open(evicted_path, "w") as f:
            f.write(reason)

        logger.warning(f"Evicted run {run_id} (idx={idx}): {reason}")

    def discover_runs(self) -> None:
        """Detect run changes and update data structures (master only). Must be followed by synchronize_state().

        Scans for new/deleted runs, calls forgotten/discovered hooks (master only),
        and updates internal data structures. Hooks and parameter resets for all ranks
        are deferred to synchronize_state().
        """
        if not self.world.is_master:
            raise RuntimeError("discover_runs() must only be called on the master rank")
        run_ids = {run_path.stem for run_path in self.output_dir.glob("run_*")}

        # Filter out evicted runs
        evicted_runs = {run_id for run_id in run_ids if (self.output_dir / run_id / "control" / "evicted.txt").exists()}
        run_ids = run_ids - evicted_runs

        deleted_runs = self.id_2_idx.keys() - run_ids
        new_runs = run_ids - self.id_2_idx.keys()

        for deleted_run in deleted_runs:
            deleted_idx = self.id_2_idx[deleted_run]
            # Call forgotten hooks (master only) before cleanup
            for hook in self._forgotten_hooks:
                hook(deleted_idx, deleted_run)
            self._delete_run_data(deleted_run, deleted_idx)

        for new_run in new_runs:
            try:
                new_id = next(iter(self.unused_idxs))

                config = self.get_orchestrator_config(new_run)
                if config is None:
                    continue

                self._create_run_data(new_run, new_id, config)
                # Call discovered hooks (master only) after data setup
                for hook in self._discovered_hooks:
                    hook(new_id, new_run, config)
            except StopIteration:
                continue

    def synchronize_state(self) -> None:
        """Sync run state across ranks and execute hooks.

        No-op in single-process mode. Surogate handles multi-GPU internally
        in C++, so there are no other ranks to synchronize with.
        Kept as a placeholder for future multi-process support.
        """
        pass

    # =========================================================================
    # Properties and Accessors
    # =========================================================================

    @property
    def used_idxs(self):
        return sorted(self.idx_2_id.keys())

    @property
    def ready_to_update_idxs(self):
        return [idx for idx, ready in enumerate(self.ready_to_update) if ready]

    def run_dirs(self) -> list[Path]:
        return [self.output_dir / run_id for run_id in self.id_2_idx.keys()]

    def get_run_dir(self, idx: int) -> Path:
        return self.output_dir / self.idx_2_id[idx]

    def __repr__(self):
        return f"MultiRunManager(max={self.max_runs})[{self.idx_2_id.keys()}]"


# Singleton instance of MultiRunManager
_MULTI_RUN_MANAGER: MultiRunManager | None = None


def get_multi_run_manager() -> MultiRunManager:
    """Returns the MultiRunManager singleton. Must be initialized first via setup_multi_run_manager()."""
    global _MULTI_RUN_MANAGER
    if _MULTI_RUN_MANAGER is None:
        raise RuntimeError("MultiRunManager not initialized. Please call `setup_multi_run_manager` first.")
    return _MULTI_RUN_MANAGER


def setup_multi_run_manager(
    output_dir: Path, max_runs: int, device: torch.device, lora_config: LoRAConfig | None = None
) -> MultiRunManager:
    """Initialize the MultiRunManager singleton.

    Args:
        output_dir: Directory containing run outputs
        max_runs: Maximum number of concurrent runs
        device: Device for LoRA tensors
        lora_config: Optional trainer LoRA config. If provided, registers validation
            and scaling hooks for LoRA rank and alpha.

    Returns:
        The initialized MultiRunManager instance.
    """
    global _MULTI_RUN_MANAGER
    _MULTI_RUN_MANAGER = MultiRunManager(output_dir, max_runs, device)

    # Register validation and scaling hooks for LoRA
    if lora_config is not None and _MULTI_RUN_MANAGER.world.is_master:
        trainer_lora = lora_config

        def validate_lora_rank(orch_config: "OrchestratorConfig") -> tuple[bool, str]:
            # Default to trainer's rank/alpha if not specified
            if orch_config.model.lora.rank is None:
                orch_config.model.lora.rank = trainer_lora.rank
            if orch_config.model.lora.alpha is None:
                orch_config.model.lora.alpha = trainer_lora.alpha
            if orch_config.model.lora.rank > trainer_lora.rank:
                return (
                    False,
                    f"model.lora.rank ({orch_config.model.lora.rank}) exceeds trainer max rank ({trainer_lora.rank})",
                )
            return True, ""

        def on_run_discovered(idx: int, run_id: str, orch_config: "OrchestratorConfig") -> None:
            _MULTI_RUN_MANAGER.scaling_factors[idx] = orch_config.model.lora.alpha / orch_config.model.lora.rank

        _MULTI_RUN_MANAGER.register_config_validation_hook(validate_lora_rank)
        _MULTI_RUN_MANAGER.register_discovered_hook(on_run_discovered)

    return _MULTI_RUN_MANAGER
